{
    "data": [
        {
            "id": "KKyvcfyF2VwhS6Z8HjAd",
            "title": "微软研究院开源 AIOpsLab：一个 AI 驱动的云运维框架",
            "image": "https://static001.infoq.cn/resource/image/d4/b6/d406e9117780b815b2a0096f486be1b6.jpg",
            "description": "<div><p>微软研究院推出 AIOpsLab 开源框架，旨在推进云运维中 AI 智能体的开发和评估。该工具提供了一个标准化且可扩展的平台，应对复杂的云环境中所面临的故障诊断、事件缓解和系统可靠性等方面的挑战。</p><p></p><p>随着微服务和无服务器架构在企业 IT 中成为标准，其复杂性带来了新的运维挑战。停机可能会影响关键业务运营，这凸显了维护系统可用性工具的重要性。许多现有的解决方案依赖专有服务或临时的手段，可能缺乏灵活性和一致性。AIOpsLab 提供了一个标准化的框架来评估和增强不同云环境中的 AIOps 智能体，有效解决了这些问题。</p><p></p><p>AIOpsLab 引入了几个关键组件来实现其目标。该框架的核心是 Agent-Cloud Interface（ACI），它通过一个协调器将 AI 智能体与应用服务分离。这个协调器负责定义任务、验证操作，并与 API 交互执行问题解决策略。任务还通过动态工作负载和故障生成器得到进一步增强，能够模拟资源耗尽、级联故障等真实运维场景。</p><p></p><p><img>https://static001.geekbang.org/wechat/images/cd/cd2c2e50da55d892a5ae38a7d419c951.png<img></p><p></p><p>来源：微软博客</p><p></p><p>这一接口概念引发了社区的广泛关注。雀巢解决方案架构师 Marco Casula分享了他的看法：</p><p></p><p></p><p>AIOpsLab 支持包括事件检测、根本原因分析和缓解在内的一系列运维任务，既是一个基准测试工具，也是一个训练环境。研究人员可以利用它在可复现的条件下评估 AIOps 智能体的性能，同时利用其模块化设计将框架扩展到新的应用场景中。</p><p></p><p>AIOpsLab 还整合了 React、Autogen 和 TaskWeaver 等流行的智能体框架，让广泛的开发者社区更易于访问。其故障注入功能能够详细测试系统间的依赖关系，提高云服务的弹性。</p><p></p><p>此外，AIOpsLab 遵循微软的安全标准和负责任的 AI 原则。未来计划与生成式 AI 团队合作，将 AIOpsLab 纳入评估前沿模型的基准体系。</p><p></p><p>AIOpsLab 已在 GitHub 上开源，基于 MIT 许可。</p><p></p><p><strong>原文链接</strong>：</p><p></p><p></p><p></p></div>",
            "link": "https://www.infoq.cn/article/KKyvcfyF2VwhS6Z8HjAd",
            "pub_date": "2025-02-03 06:00:00",
            "source": "infoq",
            "kind": 1,
            "language": "zh-CN"
        },
        {
            "id": "7KAtH65wwWoM1uqwcdFz",
            "title": "AI大模型、机器人上春晚！直点播收视28.17亿次，用云量达历年之最",
            "image": "https://static001.infoq.cn/resource/image/yy/c5/yy7caec25cd5ac6b45a196c0ef14d8c5.jpg",
            "description": "<div><p></p><p><color>“春晚的每次彩排，我们都会安排同学参与。”今年春晚彩排现场出现了很多程序员，这些就是阿里云的春晚项目团队。</color></p><p><color> </color></p><p><color>这是阿里云第一次作为云计算和AI的独家服务商的身份参与春晚。为保障春晚顺利进行，阿里云在北京、杭州分别设立了项目作战室，并有工程师在春晚现场支持节目制作。</color></p><p><color> </color></p><p><color>阿里云主要为春晚提供两部分技术支持：一是阿里云云转播技术支持全球华人云上看春晚，二是阿里云通义大模型技术支持春晚节目创新，创新视听效果。但无论哪种支持，都需要大量的计算资源。“今年云资源的使用量非常高，可能会达到历年春节的峰值。”</color></p><p><color> </color></p><h2>看得见的AI大模型技术</h2><p><color> </color></p><p><img>https://static001.infoq.cn/resource/image/46/84/46d55fbaa4f78502ca86652ae6428d84.gif<img></p><p></p><p><color>莫文蔚和毛不易合唱的《岁月里的花》赢得了不少网友好评。“真的好听，画面布置得也很好，后面也很惊艳。”节目演唱落下尾声时，舞台画面定格变成了一幅油画，无限岁月静好，让网友直呼“绝美”。</color></p><p><color> </color></p><p><color>油画定格的效果正是依托阿里云通义万相2.1的能力，通过AI图像生成、视频生成、图像编辑等AIGC技术，生成对应主题的元素与背景，增强节目舞美的视觉表现力。通义万相2.1模型在本月赢来了一次重磅升级，在大幅度复杂运动、物理规律遵循、艺术表现等方面实现全面提升，能够实现影视级画面生成质量，在发布后就登顶第三方权威榜单VBench。</color></p><p><color> </color></p><p><color>据介绍，通义万相使用时空全注意机制，让模型能够更准确地模拟现实世界的复杂动态；团队还引入了参数共享机制并针对文本的嵌入进行优化。在视频VAE方面，通义万相设计了一种创新的视频编解码方案，通过将视频拆分成若干块（Chunk）并缓存中间特征的方式，实现显存的使用与原始视频长度无关，从而能够支持无限长1080P视频的高效编解码。</color></p><p><color> </color></p><p><color>此外，人形机器人也在春晚炸场，艺术与科技结合得让很多网友眼前一亮。知乎网友称“这个实话说属于表现力远低于技术力。技术上两足机器人保持平衡就已经是很大的课题，但是春晚这种舞台上应该是外行人也能理解的牛逼。”</color></p><p></p><p></p><p></p><p><color>根据宇数科技介绍，表演机器人依据舞蹈要求设计动作，靠AI训练来执行16台H1激光SLAM定位，全自动走位变队形快速转、抛手绢结构复杂，还需紧凑且可靠舞台不平有缝隙。为凸显机械感的整机骨架设计，团队去掉了所有外皮壳体。</color></p><p><color> </color></p><p><img>https://static001.geekbang.org/infoq/d9/d90886bd7951845c1e65c66330256322.png<img></p><p></p><p><color> </color></p><h2>创新视觉</h2><p><color> </color></p><p><color>甄子丹领衔的《笔走龙蛇》武术表演，有网友对这个节目赞不绝口：“今年春晚最爱的节目！”这个节目中出现了“空中环绕、时空凝结”等创新视角，这就是阿里云AI在本次春晚投入最多的“子弹时间”特效视频。</color></p><p><color> </color></p><p><img>https://static001.infoq.cn/resource/image/03/d2/03bd16e45edee906ffa770yye1f813d2.gif<img></p><p></p><p><color> </color></p><p><color>“子弹时间”技术，也叫“云上AI多视角呈现”，采用了通义大模型技术增强的全新转播技术，具有如下特点：</color></p><p><color> </color></p><ul><li><p><span style=\"color:#494949\">多视角。传统拍摄以单视角为主，视角的变化需要通过摄像机角度变化来实现，而多视角技术利用相机阵列进行多角度拍摄，力求360度全方位保留节目细节；</span></p></li></ul><ul><li><p><span style=\"color:#494949\">3D建模还原细节。利用云计算和AI大模型算法进行3D建模、3D重建，完整还原节目的细节。</span></p></li></ul><ul><li><p><span style=\"color:#494949\">虚拟运镜呈现最终效果。利用智能虚拟运镜手法实现“时空凝结、移步异景”等立体视效。</span></p></li></ul><p><color> </color></p><p><color>实际上，这并不是一个全新的技术，央视也并非第一次使用。</color></p><p><color> </color></p><p><color>以往的“子弹时间”技术主要基于视频插帧技术（2维图像处理技术）和深度图像渲染技术（传统3维处理技术），本次春晚则主要使用了三维重建与神经体渲染相结合的技术，分别对场景进行稀疏表达和稠密表达，这两种技术同属于空间智能技术，是通义AI能力的一个重要组成部分。</color></p><p><color> </color></p><p><color>阿里云视频云负责人致凡表示，与之前相比，阿里云这次使用了更少的设备（摄像头），在云端完成了整个制作过程，真正发挥了云计算的处理能力。</color></p><p><color> </color></p><h2>30台相机采集数据</h2><p><color> </color></p><p><color>阿里云和央视总台联合首次在春晚演播大厅部署由30台相机阵列组成的云上多视角拍摄系统。项目组进场前设计了多套解决方案，并提前将这些方案在3D模型中进行充分的场景模拟和效果比对，从而使导演组能够快速选定将30台4K超高清相机架设在距离舞台又高又远的位置，推出全新的“演播厅天空环绕视角”。</color></p><p><color> </color></p><p><color>子弹时间的拍摄视角是从近乎三层楼的高度进行拍摄，这种全新视角可以根据不同节目需求灵活切换角度，还避免了对舞台布局的干扰。而且，这种高空拍摄方式更加灵活，无需频繁拆卸设备，降低了成本。</color></p><p><color> </color></p><p><color>致凡坦言，相机问题面临的挑战要比奥运会更大。</color></p><p><color> </color></p><p><color>春晚节目非常丰富，但不同节目对灯光、景别等要求差异很大。团队需要监控每个节目的灯光变化并动态调整相机的参数，否则就会出现闪烁、不同步等问题。团队还需要根据不同的节目特点重新调整镜头设置，比如舞蹈类节目要聚焦在中心位置，而技巧类节目中演员位置会比较高。这是一个不断优化的过程，一步步接近导演想要的效果。</color></p><p><color> </color></p><p><color>30台摄像机并非专业广播级摄像机，而是普通的商业级别设备。这带来成本、灵活度优势的同时，也让像相机快门、光圈、焦距等会容易受到舞台上其他摄像头影响。因此，团队做了大量的工作，包括背后的软件监控和检测，发现有弧光等问题的镜头则将其删掉。</color></p><p><color> </color></p><p><color>另外，相机设备还有过热的问题。奥运会比赛有休息时间，但春晚彩排时为保证所有设备都能正常运行，相机一直处于高负荷状态，很容易过热。因此，团队为设备增加了自适应功能，以此判断一些关键节点是否过热，如果存在就让其自动重启。这种方法确保直播时不会因为设备问题而错过重要画面。</color></p><p><color> </color></p><p><color>相机数量越少，要重建出高准确度、高精确度画面的难度就越大，对算法的要求也就越高。这次春晚上的30台摄像机相对巴黎奥运会而言，硬件数量缩减70%，这是团队在相机数量、算法优化和成本之间平衡的结果。如果相机数量再减少，虽然成本会降低，但其他地方可能会出现瑕疵；如果再增加相机数量，算法的复杂度会降低，但成本就会变高。</color></p><p><color> </color></p><p><color>更重要的是，超高速相机在拍摄子弹时间视频时，必须确保每个相机采集到的画面在时间上绝对同步，这样 3D算法才能实现理想的视觉效果。团队采用了多种技术手段，包括使用时间戳软件、调整硬件设备以及相机的参数指标，来确保所有相机在任何时刻都能同步。</color></p><p><color> </color></p><h2>算法难度增大</h2><p><color> </color></p><p><color>“奥运会用的算法可以看作版本2，春晚后则进入版本3。随着版本不断迭代，算法通用性大幅提升，这也是我们的目标。”通义实验室应用视觉实验室负责人薄列峰说道。</color></p><p><color> </color></p><p><color>算法优化过程中，团队增加了获取三维表达的时间。这一时间的增加与分辨率等具体需求密切相关。在春晚场景下，团队通过并行优化技术，如多卡并行处理，满足春晚对算法推理时间的要求。</color></p><p><color> </color></p><p><color>与奥运会中的子弹时间不同，春晚项目涉及多种物体，包括透明物体和丝带型物体等，这些都对算法提出了挑战。团队的很大部分时间用在了研发自有算法解决方案上。这个过程中，团队会针对某一类问题进行算法优化，比如对传统服饰中的丝带类元素而非丝带本身。</color></p><p><color> </color></p><p><color>视觉效果上，阿里云采用了AI大模型的3D建模和3D渲染技术，并结合本地传统的视觉特效算法进行叠加，改变了以往单一的镜头轨迹，还加入了动静结合的元素，比如镜头的移动、变焦、快放和慢放等，让画面更加丰富多样。</color></p><p><color> </color></p><p><color>进行3D建模时，输入的数据是30个相机拍摄的照片。在建模过程中，首先要保证模型的完整度和真实度，观看时不能出现视觉上的阴影、模糊或不自然的地方。团队为此进行大量的算法优化，使模型看起来更加自然。</color></p><p><color> </color></p><p><color>本次使用的技术方案还允许用户在时间和空间两个维度上更加自由地运镜，实现以往专业摄影师才能做到的高级运镜效果。而最终的效果由导演来审核。只要创意与导演想法一致，算法完全可以保障最终效果达到要求。</color></p><p><color> </color></p><p><color>技术团队和春晚导演组之间有一个互相交流和打磨的过程。前期，技术团队会先给导演看视频小样，导演充分了解技术团队能力后会提出新的需求，技术团队再针对这些需求迭代算法或增加新效果。</color></p><p><color> </color></p><p><color>不错的视频效果会得到导演组的赞许，导演之间也会进行互相交流和推荐，比如有的武术导演会把这种效果推荐给舞蹈导演。这也是致凡在参与春晚项目中印象深刻的地方。</color></p><p><color> </color></p><p><color>“这种互动能够促进我们和导演之间的合作，我们的能力会激发导演的灵感，让他们突破传统拍摄视角、尝试新的方式。每次合作，他们都会有一些新的想法。这是一个互相交流、互相促进的过程。”致凡说道。</color></p><p><color> </color></p><p><color>实际上，所有效果都在云上完成，制作速度非常快。无论是要改变、删除还是增加一个效果都能迅速实现，导演可以很快看到反馈并提出想法和意见。</color></p><p><color> </color></p><p><color>致凡表示，“最后几次排练时，导演已经对我们非常熟悉了，他甚至能自己想象出一些想要的效果，有的导演还会和我们一起挑选高燃片段。”致凡说道，“对于央视来说，这也是一种比较灵活的安排，我们的存在并不会影响到他们彩排。”</color></p><p><color> </color></p><h2>播出背后的保障</h2><p><color> </color></p><p><color> </color></p><p><color>这次蛇年春晚在境内新媒体端的实时直播收视次数和互动量均创新纪录。据初步统计，新媒体端直点播收视次数28.17亿次，比去年同比增长6.9亿次。“竖屏看春晚”直播播放量4.96亿次，较去年同时段提升18.09%，直播用户人数达2.86亿人，较去年同时段提升14.4%。</color></p><p><color> </color></p><p><color>春晚对节目画质和效果的要求更高，需要全链路4K HDR的画质、500M的超高清码率。从数据量上来说，它比巴黎奥运会的传输要求大得多，这对网络传输和云端计算的挑战也很大。</color></p><p><color> </color></p><p><color>阿里云团队通过HDR处理和智能编码等算法的结合，最终达到央视的播出效果。</color></p><p><color> </color></p><p><color>在传输和采集过程中，每个相机都需要同步采集并传输数据，这些数据量本身非常大。而且，处理的视频都是4K分辨率的，全部采用4K HDR格式，这意味着当30台相机同时传输视频，每台相机每秒30帧的视频数据汇聚在一起后，处理的计算量会非常大，网络传输速率的要求也会非常高，另外还需要不断存储这些数据。阿里利用云的弹性资源以及GPU算力来应对这种大规模的网络传输和存储需求。</color></p><p><color> </color></p><p><color>值得注意的是，真正的4K画面和真实视角并不是通过简单的生成技术就能实现，而是需要实实在在地拍摄。AI的作用主要是处理拼接以及解决重建过程中的各种效果问题。</color></p><p><color> </color></p><p><color>巴黎奥运会的子弹时间视频是纯实时生成的。而这次春晚采用的是4K技术，数据量比以往大很多。因此，阿里云团队采用了两套方案：第一套是按照央视节目的要求，整体采用4K技术；另外则是将视频降低到1080p分辨率，制作一些简单效果，给导演进行预览。导演可以通过预览大致了解效果并提出意见，技术团队则根据意见进行调整。</color></p><p><color> </color></p><p><color>“阿里云在云端使用弹性资源，可能会用到几十张GPU卡来进行处理，虽然现在还不能做到完全实时，但相比以前，时效性已经快了很多，只是最终效果的生成时间会稍长一些。当然，最终的成片还是要在云端用4K技术制作。”致凡说道。</color></p><p><color> </color></p><p></p><p><img>https://static001.geekbang.org/infoq/f4/f456f388d983ac87fc41a4ee798b3fb0.png<img></p><p></p><p><color> </color></p><h2>运维智能体上线工作</h2><p><color> </color></p><p><color>随着智能手机和各类新媒体APP的普及，越来越多的人开始通过手机开看春晚。央视对于阿里云的技术要求是“丝滑顺畅、万无一失”。为此，阿里云通过“云转播”的技术方式来实现，让手机达到电视大屏看春晚的效果。</color></p><p><color> </color></p><p><color>央视总台联合阿里云推出“云来云往”新媒体发布平台，成为2025年春晚直播信号远程分发的主要方式。该平台是央视总台和阿里云在2021年时就共同构建的，基于阿里云的全球基础设施构建国内和全球转播服务源站，依靠跨境CEN缩小远程信号传输延迟提升网络可靠性，最后通过3200多个边缘节点把直播流畅地传送到全球相关持权转播机构。</color></p><p><color> </color></p><p><color>今年春晚的直播画面，通过阿里云部署在全球的云基础设施，从北京分发到全球各地的持权转播机构，同时满足8K电视大屏、移动小屏等多终端需求，并提供竖屏春晚、三维菁彩声等观看体验。</color></p><p><color> </color></p><p><color>为保障春晚直播的稳定与流畅，阿里云建立了一套专属的重保护航机制：</color></p><p><color> </color></p><ul><li><p><span style=\"color:#494949\">人力上，阿里云在北京、杭州两地工程师进行现场24小时值班，为春晚提供不间断的重保。</span></p></li></ul><ul><li><p><span style=\"color:#494949\">技术上，将稳定性保障体系与云的弹性、高可用等能力相融合，建立从平台到业务的全链路可观测能力、弹性能力、智能告警及应急处置能力等，并将现有技术机制与云维小智（运维智能体）相结合。通过智能体与平台的深度互动，提升云平台的可运维性、稳定性，更好地辅助现场工程师提前发现风险并提升应急处置效率。</span></p></li></ul><p><color> </color></p><p><color>值得注意的是，云维小智是阿里云在2024年9月份发布的面向云平台运维场景打造的智能体 ，基于“Qwen2”基础模型实现，利用更全面、更智能的知识库进行业务领域的知识运营。据介绍，云维小智推荐的运维方案可以覆盖日常45%的场景，此前已经在国家电网和交行开始部署。</color></p><p><color> </color></p><p><color>本次春晚中，阿里云还在央视首次采用“上行链路双源站多活，下行链路多域名容错”的先进技术架构，从总台到不同地域的双源站同时推流，确保任何时刻至少有一个源站能正常工作，并采用两个反亲和节点资源的直播域名提供拉流服务，让直播具备全链路容错能力，任何环节故障都不会影响直播，整体可用性能超过了广播电视专业级传输要求。</color></p><p><color> </color></p><h2>结束语</h2><p><color> </color></p><p><color>春晚已经逐渐成为各大技术厂商的试金石。作为一个重大且不容有失的项目，背后的技术人员投入了非常多的精力和时间，但大家更享受项目成功的喜悦。</color></p><p><color> </color></p><p><color>“我们的技术能够满足导演的高需求，为观众提供一场增强的视觉体验，这种体验是现实中或通过其他技术很难获得的。”薄列峰说道。致凡也表示，“这是一个虽然非常辛苦，但很有意义的项目。”</color></p><p><color> </color></p><p><color>期待明年的技术人助力下一场更加精彩的春晚。</color></p><p><color> </color></p><p><color> </color></p><p><color>采访嘉宾：</color></p><p><color>薄列峰，通义实验室应用视觉实验室负责人，负责云上多视角拍摄“子弹时间”AI技术</color></p><p><color>致凡，阿里云视频云负责人，负责云上多视角拍摄“子弹时间”云技术支撑</color></p></div>",
            "link": "https://www.infoq.cn/news/7KAtH65wwWoM1uqwcdFz",
            "pub_date": "2025-01-29 06:00:00",
            "source": "infoq",
            "kind": 1,
            "language": "zh-CN"
        },
        {
            "id": "D4Xh19icuqHftkjXY976",
            "title": "平台抽象拯救Reddit： Kubernetes配置变化不再是难题",
            "image": "https://static001.infoq.cn/resource/image/5c/aa/5c49942be032b2a3213ee7926fa048aa.jpg",
            "description": "<div><p><color>三年前，Reddit 的基础设施工程师团队大部分时间都在忙于救火。本文谈的就是他们如何通过开发一个平台抽象来简化运维并重新掌控局面的故事。</color></p><p></p><p><color>Reddit 于 2022 年 3 月 13 日瘫痪的事件是一个粗暴的警示，提醒这家公司需要以新的方式管理它的基础设施。</color></p><p><color> </color></p><p><color>臭名昭著的“Pi Day”全站中断事件持续了刚好 314 分钟。事件源于从 Kubernetes 1.23 到 1.24 的集群范围升级操作，该升级导致了一些微妙的不可预测行为，迫使基础设施团队进行回滚，这本身就是一项高风险行动。</color></p><p><color> </color></p><p><color>即使在那时，公司工程师也知道运维方式需要改变了。</color></p><p><color> </color></p><p><color>这个广受欢迎的社交新闻论坛彼时正在扩展其服务器堆栈，使服务器跨多个可用区以提高可靠性，最终目标是在全球范围内提供服务。其他一些围绕广告投放和机器学习的项目也面临着各自的挑战。此外，公司高管正在为 IPO 做准备。</color></p><p><color> </color></p><p><color>Reddit 基础设施团队的高级软件工程师 Karan Thukral 那时指出，该公司需要一个新的平台抽象。随着公司的发展，他们需要新的平台抽象来继续高效运维。</color></p><p><color> </color></p><p><color>Thukral 与 Reddit 软件工程师 Harvey Xia 在最近的 KubeCon+CloudNativeCon 北美会议上发表了演讲，介绍了他们的基础设施团队如何创建新的平台抽象，从而提前做好计划，摆脱被动的救火模式。</color></p><p><color> </color></p><p><color>“从技术上讲，我们做到了用更少的人手来解决更具挑战性的问题，”Thukral 说。“由于我们在过去几年中投入了大量资源，我们得以有更多的专门工程时间来主动解决问题，而不是被动地救火。”</color></p><h2>神秘的命名空间</h2><p><color>2022 年，该公司运行着 20 个 Kubernetes 驱动的生产集群。基础设施团队有 92 名工程师，比公司部署的 706 名应用工程师少得多。他们的大部分工作都是帮助应用工程师。</color></p><p><color> </color></p><p><color>一个问题是命名空间的创建。在 Reddit 中，每个在 Kubernetes 上运行的应用程序都需要一个命名空间，可以通过 Helm 图表或 Kustomize 清单指定。应用程序开发人员并不是编写这些规范的专家。</color></p><p><color> </color></p><p><color>这导致了大量剪切和粘贴操作。结果错误悄然而至，而审阅者并不总能发现这些错误。这块的审查流程为应用审查过程增加了额外的 24 小时。</color></p><p><color> </color></p><p><color>Thukral 说，由于时区差异，可能需要将近一周的时间才能获得命名空间。而且，糟糕的配置设置仍然渗透到了整个持续集成流程中，这可能会导致停机故障。</color></p><p><color> </color></p><p><color>Reddit 对命名空间的管理并不一致。 Thukral 表示：“我们无法推断命名空间的来源，无法推断它是否正在使用。”因此公司制定了一条不销毁任何命名空间的规则，因为工程师无法判断命名空间是否仍在某个地方使用。</color></p><p><color> </color></p><p><color>并且所有过时的命名空间（随着公司将其单体应用程序迁移到微服务中，这样的命名空间很多）仍然占用着 Kubernetes 资源。</color></p><h2>手工集群和受困扰的基础设施</h2><p><color>与此同时，基础设施团队也面临着自己的挑战，Xia 说道。</color></p><p><color> </color></p><p><color>工程师需要 30 多个小时才能启动一个集群，其中包括 100 多个步骤，诸如配置网络、配置硬件或选择云供应商、安装控制平面以及添加可观察性和自动扩展工具等等。</color></p><p><color> </color></p><p><color>此外，正如 Pi Day 所警示的那样，集群的就地升级是不稳定的，而且根本没有退役集群的流程。仍在运行的集群的配置逐渐偏移，越来越特别，还没有文档记录这些变化。</color></p><p><color> </color></p><p><color>Xia 说，退役一个集群相当于“进行成本高昂的考古搜索，以找到必须退役的所有基础设施”。</color></p><p><color> </color></p><p><color>“这是一个自我强化的低效率循环”——Reddit 的 Harvey Xia</color></p><p><color> </color></p><p><color>那时，该组织没有很好的方法来管理整个集群。 Xia 表示，有时，基础设施会偏离指定配置太远，以至于被称为“闹鬼”。</color></p><p><color> </color></p><p><color>“任何工程师都很难自信地推断出集群应该如何运行，或者正在如何运行，这使得所有生命周期操作都极其危险。”Xia 说道。</color></p><p><color> </color></p><p><color>因此，基础设施团队的时间主要用来保持一切内容的运行和修复中断。</color></p><p><color> </color></p><p><color>Xia 表示，这种“被动、救火的心态”使得“想象、规划或构建更可持续的未来”变得非常困难。</color></p><h2>Reddit 选择 K8s 控制器而不是 IaC 的原因</h2><p><color>一个抽象会将用户的关注点与底层实现分开来，从而隐藏复杂性。根据 Xia 的定义，平台是开发人员可以在其基础上构建的可组合工具生态系统。它以公司开发人员的最佳实践为依据。</color></p><p><color> </color></p><p><color>Reddit 决定通过一组由 Kubernetes 控制流程支持的声明性 API 来实现一个平台。它的接口被定义为自定义资源。期望的状态是规范，观察到的状态通过当前状态来报告。</color></p><p><color> </color></p><p><color>自定义资源会启动 Kubernetes 控制器，后者将活动状态转变为期望状态来实现其 API。</color></p><p><color> </color></p><p><color>工程师最初考虑使用基础设施即代码工具，但后来改用基于 Kubernetes 控制器的工具。</color></p><p><color> </color></p><p><color>使用标准 IaC 很难表示任意业务逻辑，这是构建 Reddit 基础设施的主要要求。</color></p><p><color> </color></p><p><color>“我无法建模我要的工作流，在工作流中我需要从证书颁发机构提供 TLS 证书，将其卸载到 Amazon 证书管理器，并将其附加到负载均衡器上，”Xia 说。标准 IaC 平台也不是动态的。你不能依赖 IaC 来续订过期的证书。</color></p><p></p><p><img>https://static001.geekbang.org/infoq/fc/fcc602835f7e03bacc241c66aacdb508.png<img></p><p></p><p><color>相比之下，Kubernetes 控制器将确保当前状态始终处于所需状态下，或至少一直朝着所需的状态移动。这将包括所有生命周期操作。</color></p><h2>大脑和负载集群</h2><p><color>如今，Reddit 基础设施工程师花在集群管理上的时间更少了。他们有一组 API，可以通过“单一管理平台”管理多个集群。</color></p><p></p><p><img>https://static001.geekbang.org/infoq/5c/5c3fcc8a10b3a77272b27d24c1d6cba1.png<img></p><p></p><p><color>该公司有两种类型的集群。一种是控制集群，可以被认为是运维的大脑。它为其他易于替换的“负载”集群生成配置。</color></p><p><color> </color></p><p><color>“我们开始将这些集群视为牛，而不是宠物，”Thukral 说。</color></p><p><color> </color></p><p><color>这样，集群有了明确定义的属性，所有适用的配置更新都会自动流入该集群。可以执行多集群运维操作。</color></p><p><color> </color></p><p><color>每个负载集群都有自己的自定义资源，并带有属性标签，例如其运维阶段（测试、生产或准备）、地理位置和支持的云供应商。这为多集群 API 控制奠定了基础。一个联合控制器通过 K8s 标签选择器管理集群。</color></p><p><color> </color></p><p><color>集群定位是动态的。如果启动了新的测试集群，它会自动加入测试命名空间。</color></p><p><color> </color></p><p><color>“这为我们的工程师节省了大量时间，”Xia 说。</color></p><p><color> </color></p><p><color>这种方法有一些潜在的缺点。首先，编排集群是单点故障。但系统的设计使得 worker 集群即使无法更新也能继续运行和自我修复。限制可以启动的集群类型也会限制可以构建的集群种类，从而使它们整体上更易管理。</color></p><p><color> </color></p><p><color>FluxCD 用于将源配置与集群同步。Crossplane 用于将 Kubernetes API 桥接到云供应商资源。Cluster API 提供用于管理 Kubernetes 控制平面的 API。</color></p><p><color> </color></p><p><color>这些资源或基础原语都由 Reddit 自己的自定义资源来中介，允许公司在需要时交换各种实现。</color></p><p><color> </color></p><p><color>命名空间的创建也为开发人员带来了便利。现在，Reddit 应用程序开发人员无需学习 Helm 或 Kustomize，只需创建一个名为 Reddit 命名空间的自定义资源，并将其定位到一组集群即可。对于用户来说，基于角色的访问控制（RBAC）简化为两个选项：operator 或 reader 状态。</color></p><h2>没有致命弱点</h2><p><color>为了帮助基础设施工程师更轻松地创建控制器和 operator，Reddit 构建了 Achilles SDK，它建立在 Kubernetes 控制器运行时之上。</color></p><p></p><p><img>https://static001.geekbang.org/infoq/38/386841bd15c8de2afd21b9cba342c037.png<img></p><p></p><p><color>Thukral 表示，该 SDK 将整个协调循环表示为一个有限状态机。目前，这由单个函数处理，这可能会变得相当笨重。Thukral 表示，使用 Achilles 构建集群所需的步骤比在 Terraform 下组装相同集群所需的少得多。</color></p><p><color> </color></p><p><color>Achilles 自动跟踪所有子资源和状态条件。</color></p><p><color> </color></p><p><color>“我们希望让我们的基础设施工程师能够专注于构建业务逻辑，而不必成为 Kubernetes 专家，”Thukral 说。</color></p><h2>结果</h2><p><color>Reddit 仍在构建新的基础设施，不过该公司已经看到了很多成果，工程师们引以为傲。</color></p><p><color> </color></p><p><color>该平台的可扩展性更强了：不再需要为命名空间的编写和管理而头疼。安全性和应用程序栈的简单性也得到了提升。</color></p><p><color> </color></p><p><color>建立新集群的周转时间现在约为两个小时。升级可以在一小时内完成。多亏了 Achilles，可以在不到两周的时间内创建和测试控制器，“这对我们来说是一个巨大的成功，”Thukral 说。</color></p><p><color> </color></p><p><color>该公司今年年初在生产中部署了 4 个 Kubernetes 控制器，截至 KubeCon，该公司已拥有 12 个。这些控制器管理基础设施的各个方面，包括 Kubernetes Ingress 栈、AWS 网络、Redis、Cassandra、HashiCorp Vault 和 Kubernetes 本身。</color></p><p><color> </color></p><p><color>“对平台抽象的投资已获得了回报，”Xia 说。“自助服务界面已经取代了我们以前大部分的白手套流程。我们许多繁重的内部工作流程已被自动化所取代。”</color></p><p><color> </color></p><p><color>在本次演讲中，Xia 还提供了有关何时自动化工作流程的一些提示。第一步：一切都必须是可编程的。</color></p><p><color> </color></p><p><color>他说，该平台让工程师们能够专注于那些“我们希望投入主要时间解决的重大问题”。</color></p><p><color> </color></p><p><color>演讲视频：</color></p><p><color> </color></p><p><color>原文链接：</color></p></div>",
            "link": "https://www.infoq.cn/article/D4Xh19icuqHftkjXY976",
            "pub_date": "2025-01-27 00:00:00",
            "source": "infoq",
            "kind": 1,
            "language": "zh-CN"
        },
        {
            "id": "TmFdT2kYPSlvTUcCNuc4",
            "title": "云原生计算基金会宣布CubeFS毕业",
            "image": "https://static001.infoq.cn/resource/image/20/4a/20bd406d4c97227e5552426b98fcce4a.png",
            "description": "<div><p><strong>2025年1月21日 – </strong><strong>正式对外官宣CubeFS成为其毕业项目</strong>。正式从 CNCF 毕业，标志了CubeFS的技术生态受到全球业界广泛认可，云原生存储技术迈入了成熟新阶段。</p><p> </p><p>互联网技术TAC（技术决策委员会）主席邓华荣表示：“自开源以来，CubeFS持续获得了业界伙伴与用户的关注和支持，已广泛应用于AI、大数据、容器平台、存算分离等领域；作为首个由中国人主导开源的具备完整存储功能的产品，填补了行业在高性能、易运维、云原生开源分布式存储领域的空白。”</p><p> </p><p>OPPO云计算TMG（技术管理委员会）负责人、CubeFS TSC 何小春表示：“ CubeFS是OPPO在云基础设施领域的重要开源贡献，推动了云原生技术在存储场景的创新实践，为云+AI时代业务提供了关键支撑；未来我们将继续推进开源创新，同全球伙伴共建繁荣的开源生态。”</p><p> </p><p><img>https://static001.geekbang.org/infoq/50/508d2f450046350f13ca4eb912e527af.png<img></p><p><strong>毕业一览</strong></p><p><img>https://static001.geekbang.org/infoq/f2/f2d7b8932756d4ffee0cc1ae401c6a92.png<img></p><p></p><h2>CubeFS架构介绍</h2><p></p><p>CubeFS是一个开源分布式存储系统，支持POSIX、HDFS、S3等访问协议及其自身的RESTful API。它可以用于许多场景，包括大数据、人工智能、容器平台、数据库和中间件的存算分离、数据共享与备份等。</p><p> </p><p><img>https://static001.geekbang.org/infoq/32/32e21f80fc10d7e65f35f8225287fb03.png<img></p><p></p><p>作为一站式存储服务平台，CubeFS 具备多协议、可扩展元数据、混合云支持等诸多特性，拥有优秀的扩展性和容灾能力，在性能方面也表现卓越。</p><p> </p><p>CubeFS的持续开源运营，旨在以OPPO互联网后端技术及云原生实践经验反哺社区，晋升为毕业项目也意味着全球顶级技术基金会对OPPO云原生技术演进的认可，同时验证了OPPO在云基础领域的深厚积累及对未来技术趋势的深刻洞察。</p><p> </p><h2>CubeFS发展历程</h2><p></p><p>CubeFS项目成立于2017年。2019年12月被接受进入CNCF沙箱，2022年进入孵化，2024年正式毕业。</p><p> </p><p>自加入CNCF以来，用户数量增长了1900%，从10个增加到200个；社区贡献增长了1415%，从1112次增加到16845次；贡献者数量增长了1300%，从5家公司中的27人增加到42家公司中的379人。</p><p> </p><p>用户已经在生产环境中使用CubeFS多年，管理数百PB的数据，并支持数十万客户端的访问。主要用户的总存储规模预计到2025年初已经达到了350PB。该项目在商业、云存储和在线媒体流等领域得到了广泛应用，用户包括京东、OPPO、网易等。尤为值得一提的是，CubeFS所有特性均向社区开放，这使其具有更高的透明度和更强的持续性，能够更好地与不同参与方协作，促进社区的蓬勃发展。</p><p> </p><p>在2024年11月北美KubeCon大会上发布的CNCF技术景观雷达报告中，在 Batch/AI/ML 计算技术领域，开源分布式存储系统CubeFS 与 Apache Airflow、Kubeflow 等一同处于技术雷达的 “采用（adopt）” 位置，实用性评分达 +40，跻身前四，是Batch/AI/ML相关项目中的最成熟和已经被Adopt的少数项目之一。</p><p> </p><p>“像OPPO这样的组织已经开始使用CubeFS来运行大规模的机器学习平台，并进行AI训练。”CNCF首席技术官Chris Aniszczyk表示。“CubeFS的稳定性、可靠性能和活跃社区赢得了用户的信任，我们期待看到随着AI和机器学习的进一步发展，CubeFS的采用将如何演变。”</p><p> </p><h2>项目用户</h2><p></p><p>“CubeFS在OPPO被广泛应用，应用于大规模AI、大数据和数据库存算分离等业务场景。它的存储协议层支持S3、POSIX、HDFS语义，存储引擎层支撑多副本和纠删码；其特性丰富，功能强大，展现出良好的稳定性和持续迭代能力，带来了卓越的用户体验。”—— OPPO 王红岩</p><p> </p><p>“CubeFS提供了强大且可扩展的存储解决方案，给网易带来了显著的收益，增强了自身的云服务能力。该项目的稳定性和明确的范围与路线图赢得了网易的信任，使其成为云原生基础设施需求的可靠组成部分。”—— 网易 张锐</p><p> </p><p>“CubeFS在处理极端场景和满足企业需求方面表现出色。京东建议该项目可以通过更多社区活动和更广泛的推广来充分发挥其潜力。”—— 京东 张墨飞</p><p> </p><p>“CubeFS的产品功能日益完善，尤其是在AI业务场景中。在满足性能的同时，它兼具成本、可靠性和回收站等特性，能够很好地满足业务使用的需求。该项目的稳定性和业务特性技术设计文档对我们使用CubeFS非常有帮助。”—— Shopee 黄福堂</p><p> </p><p>“小米的内部存储服务利用CubeFS的许多优秀特性来应对各种复杂存储场景。得益于其卓越的架构设计，CubeFS提供了出色的可扩展性，能够支持数百亿个文件和PB级数据容量。此外，该项目拥有一个活跃的社区，提供丰富的功能和稳定可靠的服务质量。”——  小米 佳朋</p><p> </p><h2>项目维护者</h2><p></p><p>“云原生是一个非常广泛的场景，生态系统将继续改善，这也需要存储解决方案的发展。我们期待与社区合作，构建最全面的存储解决方案，以应对日益增长的存储需求。”—— CubeFS维护者 常亮</p><p> </p><p>“期待进一步增强CubeFS的能力和生态系统，确保其符合行业标准和最佳实践，并培养项目维护者，以促进项目的持续发展。” —— CubeFS维护者 曾雪伟</p><p> </p><p>“期待与社区合作，将CubeFS发展成为最可靠的分布式存储解决方案，以满足用户各种存储需求。”—— CubeFS维护者 张墨飞</p><p> </p><p>“CubeFS是首个完整的具备所有存储功能的CNCF毕业的开源项目。作为一个毕业的CNCF项目，我们可以吸引更多贡献者，推动CubeFS的更深层次迭代和演进。这将帮助我们构建一个集成的数据引擎，以解耦产品和存储引擎，让用户更好地专注于各自能力的发展。”—— CubeFS维护者 沈杰</p><p> </p><h2>未来规划</h2><p></p><p>CubeFS将继续关注产品质量、稳定性、自动化运营能力，聚焦AI、大数据、在线服务等领域，通过持续技术创新支撑业务需求，简化大规模数据存储与管理成本，同时通过分布式缓存加速、冷热分层等能力建设，增强在公有云/混合多云场景下的业务支撑能力。</p><p></p><p>了解更多关于 CubeFS的信息，请访问：</p><p>网站: </p><p>代码仓库: </p><p>文档: </p><p>Slack: </p><p>X: </p></div>",
            "link": "https://www.infoq.cn/news/TmFdT2kYPSlvTUcCNuc4",
            "pub_date": "2025-01-24 08:01:18",
            "source": "infoq",
            "kind": 1,
            "language": "zh-CN"
        },
        {
            "id": "CPer5DmbjoI57uEzorlc",
            "title": "Netflix 通过新的配置功能增强了 Metaflow",
            "image": "https://static001.infoq.cn/resource/image/f5/26/f5c6740fa39861c5b5f017e84cd8e126.jpg",
            "description": "<div><p>Netflix 对其 Metaflow 机器学习基础设施做出了一项重大改进：一个新的 Config（配置）对象，为 ML 工作流带来了强大的配置管理能力。这一新增功能解决了 Netflix 团队面临的一个共同挑战，他们管理着跨不同 ML 和 AI 用例的数千个独特的 Metaflow 流程。</p><p></p><p>Netflix Metaflow 是一个开源数据科学框架，旨在简化构建和管理数据密集型工作流的过程。它允许用户将工作流定义为有向图，这样就能很方便地对其可视化和迭代了。Metaflow 自动处理工作流的扩展、版本控制和部署操作，这是机器学习和数据工程项目中的核心工作。它为数据存储、参数管理和计算执行等任务提供了内置支持，既可以在本地也可以在云端执行操作。</p><p></p><p><img>https://static001.geekbang.org/wechat/images/af/af7f057f1e4eeced13abf40944700d9c.png<img></p><p></p><p>Metaflow 基础设施栈</p><p></p><p>新的 Config 特性代表了 Netflix 配置和管理 ML 工作流方式的根本性转变。虽然 Metaflow 一直擅长提供数据访问、计算资源和工作流编排的基础设施，但团队以前缺乏统一的方式来配置流程行为，尤其是对于装饰器和部署设置而言更是如此。</p><p></p><p>Config 对象加入了 Metaflow 现有的工件（artifact）和参数（parameter）的组合，但在时间执行上有一个关键的区别。虽然工件在每个任务结束时保留，参数在运行开始时解析，但 Config 会在流程部署期间解析。这种时间差异使 Config 在设置针对部署定制的配置方面特别好用。</p><p></p><p>可以使用人性化，容易看懂的 TOML 文件指定 Config，从而轻松管理流程的各个方面：</p><p></p><p></p><p>Netflix 的内部工具 Metaboost 展示了该配置系统的强大能力。Metaboost 是一个用于管理 ETL 工作流、ML 管道和数据仓库表的统一界面。新的 Config 功能允许团队在保持核心流程结构的同时创建不同的实验配置。</p><p></p><p>例如，ML 从业者只需交换配置文件即可轻松创建其模型的变体，从而快速试验不同的特性、超参数或目标指标。事实证明，此功能对于 Netflix 的内容 ML 团队特别有价值，该团队负责处理数百个数据列和多个指标。</p><p></p><p>新的配置系统提供了几个优点：</p><p></p><ul><li><p>灵活的运行时配置：可以混合使用参数和配置来平衡固定部署和运行时的可配置性。</p></li></ul><ul><li><p>增强的验证：自定义解析器可以验证配置，还能与 Pydantic 等流行工具集成。</p></li></ul><ul><li><p>高级配置管理：支持 OmegaConf 和 Hydra 等配置管理器，可实现复杂的配置层次结构。</p></li></ul><ul><li><p>动态生成配置：用户可以从外部服务检索配置或分析执行环境（例如当前 GIT 分支），以在运行期间将其作为附加上下文包含在内。</p></li></ul><p></p><p>这项增强功能代表了 Metaflow 作为机器学习基础设施平台发展的重要一步。通过提供更结构化的方式来管理配置，Netflix 让团队更容易维护和扩展他们的 ML 工作流程，同时遵循各自的开发实践和业务目标。</p><p></p><p>该功能现已在 Metaflow 2.13 中提供，用户可以立即开始在他们的工作流程中实现它。</p><p></p><p>一些类似 Netflix Metaflow 的工具也能帮助数据科学家和工程师管理工作流程、编排管道以及构建可扩展的机器学习或数据驱动系统。这些工具有着略微不同的需求和优先级，但它们都旨在简化复杂的工作流程和扩展数据操作。以下是一些值得一提的例子：</p><p></p><ul><li><p>Apache Airflow：一个广泛使用的开源工作流编排平台。它允许用户将任务及其依赖关系定义为有向无环图（DAG）。Metaflow 专注于数据科学管道，而 Airflow 则更通用，擅长管理跨不同领域的工作流。</p></li></ul><ul><li><p>Luigi（Spotify）：一个旨在构建复杂管道的开源 Python 框架。与 Metaflow 一样，Luigi 能处理依赖项、工作流编排和任务管理，但它不太关注机器学习方面的特定需求。</p></li></ul><ul><li><p>Kubeflow：Kubernetes 的机器学习工具包。它专门用于管理 ML 工作流并在生产中部署模型，使其成为基于 Kubernetes 的环境的不二之选。</p></li></ul><ul><li><p>MLflow：一个管理 ML 生命周期的开源平台，包括实验跟踪、可重复性、部署和监控等能力。MLflow 对模型版本控制和部署有强大的支持，但缺乏 Metaflow 的更广泛的工作流编排功能。</p></li></ul><ul><li><p>Argo Workflows：一个 Kubernetes 原生工作流引擎，旨在在容器化基础设施上运行复杂的工作流。对于已经在使用 Kubernetes 并正在寻找轻量级解决方案的团队来说，它是理想的选择。</p></li></ul><p></p><p>虽然这些工具在某些功能上有重叠，但 Metaflow 凭借其简单性、可扩展性以及对机器学习工作流程的内置支持脱颖而出，这对数据科学团队来说特别有吸引力。</p><p></p><p><strong>原文链接：</strong></p><p></p><p>Netflix Enhances Metaflow with New Configuration Capabilities()</p></div>",
            "link": "https://www.infoq.cn/article/CPer5DmbjoI57uEzorlc",
            "pub_date": "2025-01-24 08:00:00",
            "source": "infoq",
            "kind": 1,
            "language": "zh-CN"
        },
        {
            "id": "Gc9gm9wZwNVhOZXDc1Yz",
            "title": "火山引擎RTC联合乐鑫、移远：智能硬件注入“豆包”，“模”力升级",
            "image": "https://static001.infoq.cn/resource/image/75/bf/75325ce6d5261bd487a1affa7e4acbbf.png",
            "description": "<div><p>在大模型时代，硬件设备正以前所未有的速度智能化升级，成为人工智能领域成长最快的赛道之一。在2025年的国际消费电子展（CES）上，我们看到“万物皆可 AI”的景象，而实时音视频也成为了用户与AI硬件重要的交互方式。</p><p></p><p>例如，陪伴类机器人如智能儿童玩具、AI 宠物，用户可以与它们进行聊天，进行问答、倾诉或求夸夸。同时，这些机器人还能通过设备上的摄像头和视觉传感器，智能识别用户表情、动作以及周围物品和环境，从而提供更加丰富和贴心的互动反馈。此外，智能家居、教育硬件以及智能穿戴设备等产品也在不断迭代升级，AI 与硬件的结合正逐渐渗透到我们生活的方方面面，提升生活的便捷性。</p><p></p><p><img>https://static001.geekbang.org/infoq/81/81aeaed083bc4aed0240fce7b25a27c6.webp<img></p><p></p><p>大模型与应用发展迅速的当下，机遇与挑战同在，初涉大模型和硬件结合的厂商，想要在硬件设备中加入自然流畅的 AI 实时语音功能，可能会面临到一些挑战，例如：</p><p></p><p>技术复杂变化快，研发成本高：厂商如果选择自行搭建音视频传输和编排语音大模型组件，还须依据硬件芯片特性开展深度优化调试工作，整体投入大。而且，多模态融合正成为交互趋势，研发工作的复杂程度也会进一步增加。</p><p></p><p>3-5s的反馈，响应延迟优化棘手：随着硬件设备加速智能化，用户对语音交互的实时性和准确性有了更高的期待。然而，许多厂商在初步集成 AI 实时语音功能时，常常面临3到5秒的整体响应延迟。特别是在网络条件不佳（如信号弱或网速慢）的环境中，这种延迟可能会进一步延长，并且可能导致关键信息的丢失，严重影响 AI 反馈内容的质量。</p><p></p><p>交流像用“对讲机”，交互体验有待提升：市面上大多初代智能硬件的对话功能还不够成熟，用户在与 AI 互动时，需要持续按键输入，与人们日常生活中随时随地自然交流的习惯相去甚远，体验生硬如用“对讲机”。此外，AI 返回内容若不符合预期或过长，用户无法实时打断，缺乏灵活性，难以满足用户对智能硬件的期待和需求。</p><p></p><h2>> 一站式Turnkey解决方案，功能全面快速接入</h2><p></p><p><strong>火山引擎视频云 RTC 联合乐鑫、移远等物联网芯片制造商、解决方案供应商，推出「实时对话式 AI 嵌入式硬件」解决方案。</strong>硬件设备通过方案的 AI 语音交互框架即可无缝对接火山引擎 RTC 的实时通信能力和云端智能体服务，实现与豆包大模型超低时延、流畅的交互。</p><p></p><p>在端侧，芯片集成了先进的音频处理技术，包括自动唤醒功能和音频3A 等，以提升音频输入的清晰度。同时，火山引擎 RTC 提供音视频传输，并具备抗弱网特性，以及智能体管理功能，确保设备即使在网络条件不佳的情况下也能稳定通话。在云端，智能体服务则可提供 Function calling 和知识库支持，使得硬件设备能够提供个性化服务和智能决策，满足用户的深层次需求。</p><p><img>https://static001.geekbang.org/infoq/75/75c494a9726b45ada8d22677da9d917f.webp<img></p><p></p><p></p><p>现在，实时对话式 AI 嵌入式硬件解决方案已开源，无需复杂的开发流程和适配兼容，即可快速高效地为硬件设备加入 AI 实时语音功能，一天内即可完成集成跑通。</p><p></p><p><img>https://static001.geekbang.org/infoq/06/06a7d1ce8ab0ee7cfd09ee7675a3c118.webp<img></p><p></p><p></p><p></p><h2> > 升级AI互动体验，向精品“爆款”迈进</h2><p></p><p>在智能硬件市场竞争日趋激烈的当下，企业要想打造一款能够在市场上脱颖而出的爆款产品，除了产品设计要满足用户需求外，优质的用户交互体验也成为产品成功的关键。「实时对话式 AI 嵌入式硬件」解决方案致力于优化 AI 语音通话体验，让用户拥有流畅、自然、真实的 AI 互动。</p><h2>实时响应，低时延体验</h2><p>针对硬件场景，在保持极低功耗的同时，实现端到端响应延时可低至1秒，为用户提供实时的互动体验，让沟通更加丝滑。</p><h2>稳定流畅，抗弱网能力</h2><p>火山引擎 RTC 基于全球部署的实时传输网络，保障用户最后一公里的接入体验，即使在网络条件不佳，如丢包率高达80%的情况下，可保证通话稳定，并且语义信息的完整传输，不丢失任何重要内容。</p><h2>交互自然，智能打断</h2><p>用户无需通过按键或其他输入方式，即可享受自然流畅的双向通话体验。毫秒级人声检测和打断响应，支持随时精准打断，让交流更加灵活。</p><p></p><p><size>ToyCity（粑老师 IP）采用一站式方案实现 AI 语音通话</size></p><p></p><p>在2024年火山引擎冬季 FORCE 原动力大会上，火山引擎视频云携手乐鑫科技、ToyCity、FoloToy 和魂伴科技，共同推出了创新的“硬件+对话式 AI 智跃计划”，一起见证 AI+硬件加速融合。当前「实时对话式 AI 嵌入式硬件」解决方案已成功应用于 IP 玩具、AI 机器人、智能家电等诸多硬件品类，为 IP 玩具注入数字生命（如视频演示），让 AI 机器人交互更加丰富生动，使智能家电因个性化服务而更具吸引力。</p><p></p><p>随着 AIoT 市场和大模型的不断扩展，各类硬件的感知和智能水平将实现空前提升。我们期望与众多芯片制造商及智能硬件合作伙伴携手，共同推动硬件智能化的快速发展，让智能硬件不仅仅是工具，更是理解、陪伴我们的生活伙伴。</p></div>",
            "link": "https://www.infoq.cn/article/Gc9gm9wZwNVhOZXDc1Yz",
            "pub_date": "2025-01-22 09:02:50",
            "source": "infoq",
            "kind": 1,
            "language": "zh-CN"
        },
        {
            "id": "xySLtdO3qhqgN4GSCkV5",
            "title": "揭秘字节跳动内部流量调度与容灾实践【上】",
            "image": "https://static001.infoq.cn/resource/image/87/f2/8731c16673684db3ff68a9372b9430f2.jpg",
            "description": "<div><p></p><p>面临超大规模流量时，平衡好稳定性、性能、成本，能确保用户在访问服务时获得流畅、快速且可靠的体验，这对于提高用户满意度和粘性至关重要。TrafficRoute GTM 为业务提供基于 DNS 的全球流量<strong>负载均衡、智能调度、自动容灾</strong>服务，可以帮助业务提升连续性、实现资源优化、获取更多竞争优势。</p><p></p><h2>1.火山引擎 Trafficroute GTM 简介</h2><p></p><p>火山引擎 Trafficroute GTM 是基于 DNS 的流量路由服务。它依托全球1100+分布式探测节点，构建出强大的网络质量感知能力，实现了对<strong>“端-边-云”</strong>全链路流量的质量感知，从而能根据APP应用的实时的访问质量、节点负载和健康状况作出动态的流量调度。</p><p></p><p>此外，Trafficroute GTM 还提供灵活的调度策略，其中GEO-基础路由功能丰富，包括负载均衡、会话粘性和故障转移等多种特性。而Perf-智能路由则在基础路由的基础上，进一步提供性能优先和负载反馈等智能调度能力，以满足更高层次的调度需求。</p><p></p><p></p><p><img>https://static001.geekbang.org/infoq/31/317ab16a562fde98be10e0e7e1847f58.webp<img></p><p></p><p></p><p></p><p>在字节跳动内部业务中，通过对Trafficroute GTM 能力的合理运用，落地了同城多活、多云混合等经典架构，也落地了边缘下沉，边缘计算x中心云等大规模分布式场景的最佳实践。</p><p></p><h2>2.GEO-基础路由，实现流量自定义编排</h2><p></p><p>TrafficRoute GTM 的 GEO-基础路由赋予用户灵活的流量管理能力，用户可根据具体业务需求，如负载均衡、就近接入、多活/灾备等，自定义路由调度策略，通过<strong>资源（地址池）编排、健康检查编排、路由（调度）规则编排</strong>等能力，打造个性化的流量调度与容灾解决方案。</p><p></p><p></p><p><img>https://static001.geekbang.org/infoq/f6/f64b07d004250bd0ce63c96bc81f220f.webp<img></p><p></p><p></p><h2>2.1地址资源编排：地址按需分类</h2><p></p><p>资源是指流量访问的终点，包括公有云的 EIP 、CDN 的 CNAME 或边缘接入点等。TrafficRoute GTM 支持用户按照业务场景对资源(地址)进行自定义分类、组合和编排，编排形成的地址池可被路由规则引用，进而打造个性化的流量调度与容灾解决方案。</p><p></p><p></p><p><img>https://static001.geekbang.org/infoq/3b/3bc4b83e0fcc1e0c25d7d44590f04e47.webp<img></p><p></p><p></p><h2>2.2健康检查编排：全链路监测能力</h2><p></p><p>健康检查是实现自动容灾的必要条件，TrafficRoute GTM 具备覆盖<strong>全球范围</strong>的 L3/L4/L7 健康检查功能，用户可以配置不同灵敏度的全链路监测能力，以此为自动容灾提供精准的决策支持，最终实现最快分钟级自动容灾。</p><p></p><p></p><p><img>https://static001.geekbang.org/infoq/e1/e130d00a2c401d746c42674aae17cef2.webp<img></p><p></p><p></p><h2>2.3路由规则编排：流量调度和容灾</h2><p></p><p>通过精心配置 TrafficRoute GTM 的路由（调度）规则，可以精确控制流量的来源与去向，同时在发生故障时，确保流量能够按照预设的容灾方案进行故障转移。</p><p></p><p></p><p><img>https://static001.geekbang.org/infoq/70/70b792a8e8e1b9669a020150a87229cc.webp<img></p><p></p><p></p><h2>3.字节跳动流量编排内部实践</h2><p></p><p>在字节跳动内部业务中，通过 TrafficRoute GTM 的自定义流量编排实现了同城多活、异地灾备、全球多CDN调度、CDN回源调度等经典架构，帮助内部业务：</p><ul><li><p><strong>在稳定性上</strong>，将MTTR降低至分钟级，实现最快1分钟故障发现，3-5分钟90%+流量收敛（由于流量收敛时长受客户端分布、localDNS行为、是否使用长连接等多种因素影响，3-5分钟90%+为参考值)</p></li></ul><ul><li><p><strong>在性能上</strong>，通过编排，将客户流量调度至各自访问体验最佳节点上，实现访问时延降低15%+</p></li></ul><ul><li><p><strong>在成本上</strong>，通过编排，将流量优先调度至单位成本更低的资源上，实现带宽成本降低10%+</p></li></ul><p></p><p><img>https://static001.geekbang.org/infoq/d1/d13024a768bdf7b84cff2a73eeffa43e.webp<img></p><p></p><h2>3.1同城多活，异地灾备，确保业务稳定与连续</h2><p></p><p>在字节跳动业务中，同城多活与异地灾备架构是确保超大规模业务全天候稳定运行的核心策略之一。借助 GTM 的 GEO-基础路由模式，我们成功构建了<strong>AZ 间流量负载均衡、 Region 间异地灾备、客户端 GEO&ISP 就近访问、分钟级自动容灾 4大能力，</strong>以这 4 大能力为保障，实现了流量负载均衡、客户端就近接入、分钟级自动容灾等，确保了业务的稳定性和连续性。</p><p></p><p></p><p><img>https://static001.geekbang.org/infoq/69/69215d29b520f590607a8c3325a61a8d.webp<img></p><p></p><p></p><p></p><p><strong>AZ 间流量负载均衡</strong></p><p>通过编排路由规则的<strong>地址池权重</strong>，使得华北移动的客户端按照60% vs 40%比例在RegionA的AZ之间实现了<strong>负载均衡。</strong></p><p></p><p></p><p><img>https://static001.geekbang.org/infoq/4c/4c1929b765da84d349dab7c15a561521.webp<img></p><p></p><p></p><p>同时，通过打开<strong>*会话粘性</strong>开关(TOB版本待发布)，使得特定的华北移动客户端始终访问同一个EIP，实现会话保持功能。</p><p></p><p></p><p><img>https://static001.geekbang.org/infoq/0e/0e675785388934075a23ea133b9c4cfb.webp<img></p><p></p><p></p><p><strong>Region间异地灾备</strong></p><p>通过编排路由规则的<strong>生效地址池集合 </strong>vs<strong> 其他地址池集合</strong>，实现了当Region A整体故障，流量切到Region B，实现异地容灾能力。</p><p></p><p></p><p><img>https://static001.geekbang.org/infoq/21/21e6efc27be2f764fa69c3bc25b18c3e.webp<img></p><p></p><p></p><p><img>https://static001.geekbang.org/infoq/61/61a76b2af025caa66f9898df85252cc3.webp<img></p><p></p><p></p><p><strong>客户端GEO&ISP就近访问</strong></p><p>通过编排路由规则的<strong>线路</strong> & <strong>生效地址池集合</strong>，实现了根据客户端<strong>地理位置</strong>和<strong>运营商</strong>来就近访问服务的能力，从而确保时延最低、性能最优、体验最佳。</p><p></p><p></p><p><img>https://static001.geekbang.org/infoq/a0/a07c5fece7cfe214267fcc53ddadd88a.webp<img></p><p></p><p></p><p><strong>分钟级自动容灾</strong></p><p>通过编排健康检查规则，实现了<strong>最快1分钟</strong>故障感知&容灾切换，<strong>3-5分钟 </strong>90%+流量收敛的能力；且可感知全链路的故障，覆盖客户端->运营商网络->机房入口->后端服务。</p><p></p><p></p><p><img>https://static001.geekbang.org/infoq/c6/c6a9733cf32618ef6efb932e849aca8b.webp<img></p><p></p><p></p><p><img>https://static001.geekbang.org/infoq/fd/fd4867270abdf79c87cc2c98e661165f.webp<img></p><p></p><p></p><p>以下是字节跳动内部自动容灾实践案例，某个面向企业级客户的边缘云与中心云混合部署业务，在凌晨3点47分遭遇了某机房故障，GTM在3点48分迅速检测到这一故障，并自动启动了容灾机制。到了4点左右，中心机房已成功收敛了90%的流量。随后，在4点02分，GTM系统监测到故障已恢复，随即自动将流量回切至边缘机房。</p><p></p><p></p><p><img>https://static001.geekbang.org/infoq/32/32372b01fe0ce49c70f6424732c1eae6.webp<img></p><p></p><p></p><p></p><p>更多关于GTM【同城多活】【异地灾备】【两地三中心】等技术架构可参考往期分享：。</p><p></p><p></p><h2>3.2全球多CDN调度，始终选用“最优” 加速厂商</h2><p></p><p>字节跳动的业务，会在全球应用包括火山引擎 CDN 在内的多家 CDN 厂商，来实现资源加速。然而，不同 CDN 厂商的服务能力存在差异，即便是同一厂商，在不同地区或不同时间段的表现也有所不同。因此，确保在全球各个地区始终选用“最优” CDN 厂商，成为了一项重要需求。</p><p></p><p>借助 TrafficRoute GTM 的流量编排，业务方能够根据不同区域的需求，灵活选择“资源覆盖更广”或“加速性能更好”的 CDN 厂商进行内容加速，同时可在多个厂商之间权衡成本效益(基于P95带宽计费)。同时， TrafficRoute GTM 确保了不同区域的流量能够“就近接入”选定的 CDN 厂商，以保障接入性能。在国内，针对不同运营商采用了定制化的路由策略，避免了 ISP 间的“跨网”问题，确保网络流畅。</p><p></p><p>为了应对流量管理中的潜在故障，每个流量部分都配置了多个地址池（即多个厂商），以实现故障时的自动流量切换。在故障探测方面， TrafficRoute GTM 采用了智能推荐的分布式综合探测方法，确保探测点与流量来源处于同一区域，减少误判风险，并具备分钟级的故障感知和流量迁移能力。</p><p></p><p>此外，故障转移（Failover）算法遵循“快速迁移，慢速恢复”的原则，结合历史流量质量监测和防抖动算法，以优化策略执行，确保服务的连续性和稳定性。</p><p></p><p></p><p><img>https://static001.geekbang.org/infoq/4c/4c33471d9c3b0c7454612567d9aad4cb.webp<img></p><p></p><p></p><h2>3.3 CDN回源调度，保障源站可用性</h2><p></p><p>字节跳动内部业务中，每时每刻都有超大规模的视频、图文、API等流量经过 CDN/DSA 加速回到源站，因此源站的可用性至关重要。</p><p></p><p>为确保源站的可用性，我们通过 GTM 将源站接入点（一般是若干EIP）封装成<strong>回源域名</strong>，回源域名被加载到 CDN 厂商的回源配置当中。由于回源域名由 GTM 托管，因此其具备了<strong>流量负载均衡、全链路健康检查、分钟级自动容灾</strong>等能力，保证了 CDN 厂商至源站这一<strong>回源链路的高可用性</strong>。这一容灾策略与 CDN 厂商回源机制本身包含的容灾策略相融合，可构建更加健壮的回源链路，进一步确保链路的高可用性。</p><p></p><p></p><p><img>https://static001.geekbang.org/infoq/c1/c1ccbce9a5bd95fd7330bb8bd7493517.webp<img></p><p></p><p></p><p>在确保源站的可用性的过程中，通过 GTM 实现了两大能力，一是源站负载均衡，二是源站自动容灾。通过在GTM回源域名上配置源站的权重，可以实现将CDN回源流量在不同站点之间进行分配，保障了源站负载均衡；GTM通过周期性http健康检查，实时感知源站运行状态，无论是部分节点异常还是源站整体故障，GTM都能在1分钟内感知并完成容灾切换，实现了源站自动容灾。</p><p></p><p>以下是字节跳动内部CDN回源切流实践案例，通过GTM在14点08分将源站的telecom线路权重调整为0，14点12分左右源站telecom线路流量切空，14点42分左右通过GTM复原源站的telecom线路权重，14点45分左右源站telecom线路实现流量100%收敛。</p><p></p><p></p><p><img>https://static001.geekbang.org/infoq/e6/e69e734d1002a67964429df20693248e.webp<img></p><p></p><p></p><p>具体配置GTM落地CDN回源调度的流程，可<strong>点击文末【阅读原文】</strong>，跳转<strong>《通过 GTM 在 CDN 中实现自定义的负载均衡、健康检查和容灾切换》</strong>一文。</p><p></p><h2>END</h2><p></p><p>通过搭建同城多活、异地灾备；全球 CDN 调度；CDN 回源调度等经典架构，Trafficroute GTM 帮助字节跳动内部业务经受了超大规模流量考验，确保始终为用户提供稳定的服务。</p></div>",
            "link": "https://www.infoq.cn/article/xySLtdO3qhqgN4GSCkV5",
            "pub_date": "2025-01-22 08:11:43",
            "source": "infoq",
            "kind": 1,
            "language": "zh-CN"
        },
        {
            "id": "o0vBzBidAUlWyBltH2yd",
            "title": "Gitpod Flex，替代 Kubernetes 的云开发环境",
            "image": "https://static001.infoq.cn/resource/image/f0/f8/f0b850265d7fabd59461693ec37403f8.jpg",
            "description": "<div><p>Gitpod 是一个云开发环境平台。在经过六年的使用和试验后，他们最近决定放弃 Kubernetes。这一决定源于他们管理 150 万用户的开发环境，同时每天处理大量环境的经验。</p><p></p><p>Gitpod 的首席技术官兼联合创始人 Christian Weichel 和高级工程师 Alejandro de Brito Fontes 在一篇博文中详细阐述了这一决定的历程。Gitpod 发现，虽然 Kubernetes 非常适合生产负载，但用于开发环境时会带来重大挑战。</p><p></p><p>开发环境的天然性质也是这些挑战的成因。与生产负载不同，开发环境具有高度的状态性和交互性，开发人员深度参与其源代码和更改过程。它们显示出了不可预测的资源使用模式，需要复杂的权限和功能，通常需要 root 访问权限和安装各种包的能力。这些因素使开发环境不同于典型的应用程序负载，并为 Gitpod 的基础设施决策提供了灵感。</p><p></p><p>最初，Kubernetes 因其可扩展性、容器编排和丰富的生态系统而成为 Gitpod 基础设施的理想选择。然而，在扩展时，他们遇到了许多挑战，特别是在安全和状态管理方面。先是资源管理出现了挑战，每个环境的 CPU 和内存分配尤其成问题。开发环境中 CPU 需求激增的情况使得管理人员很难预测何时用户何时需要使用 CPU 时间，这还引发了平台对 CPU 调度和优先级进行的许多实验。</p><p></p><p>存储性能优化是另一个关键领域。Gitpod 尝试了各种设置，包括 SSD RAID 0、块存储和持久卷声明（PVC）。每种方法在性能、可靠性和灵活性方面都有其权衡。备份和恢复本地磁盘被证明是一项昂贵的操作，需要仔细平衡 I/O、网络带宽和 CPU 使用率。</p><p></p><p>自动扩展和启动时间优化是 Gitpod 的重要目标。他们探索了各种扩大规模和向前发展的方法，包括“幽灵工作区”、ballast pod，最后则是集群自动缩放器插件。镜像拉取优化是另一个关键方面，Gitpod 尝试了多种策略来加速镜像拉取，包括守护进程预拉取、最大化层重用和预烘焙镜像。</p><p></p><p>Kubernetes 中的网络引入了其自身的一系列复杂性，特别是在开发环境访问控制和网络带宽共享方面。安全性和隔离带来了重大挑战，因为 Gitpod 需要提供安全的环境，同时为用户提供开发所需的灵活性。他们实现了一个定制的用户命名空间解决方案来解决这些挑战，其中涉及很多复杂组件，例如文件系统 UID 转换、安装屏蔽进程和自定义网络功能。</p><p></p><p>Hacker News 上有一场与 Gitpod 之旅相关的有趣 对话。HN 用户之一 datadeft 在回复中引用了原始 k8s 论文并说，</p><p></p><p></p><p>为了寻找更好的解决方案，Gitpod 尝试了微型虚拟机技术，如 Firecracker、Cloud Hypervisor 和 QEMU。虽然这些技术提供了一些不错的功能，例如增强的资源隔离和改进的安全边界，但它们也在开销、镜像转换和技术特有的约束方面带来了新的挑战。</p><p></p><p>最终，Gitpod 得出结论，使用 Kubernetes 实现他们的目标是可能的，但在安全性和运营开销方面需要权衡。这种结论促使他们开发了一种新的架构 Gitpod Flex，它继承了 Kubernetes 的重要优势，例如控制理论和声明性 API，同时简化了架构并改善了安全基础。</p><p></p><p>Gitpod Flex 引入了与开发环境相关的抽象层，并去除了许多不必要的基础架构。这种新架构允许用户顺利集成开发容器，并能够在台式机上运行开发环境。它可以在任意数量的区域中快速部署自托管，从而更好地控制合规性并在建模组织边界时提供灵活性。</p><p></p><p>总之，Gitpod 的历程强调了系统选择决策的重要性。选择系统时，必须考虑其改善开发体验、降低运营负担和提高利润的能力，而不是简单地在 Kubernetes 和替代方案之间进行选择。要了解有关 Gitpod Flex 架构的更多信息，感兴趣的读者可以观看这个深度分享。</p><p></p><p>原文链接：</p><p></p><p>Gitpod Flex, Cloud Development after Kubernetes()</p></div>",
            "link": "https://www.infoq.cn/article/o0vBzBidAUlWyBltH2yd",
            "pub_date": "2025-01-21 08:00:00",
            "source": "infoq",
            "kind": 1,
            "language": "zh-CN"
        },
        {
            "id": "SiC0YrEuk0pbCwPavWb2",
            "title": "对 OpenAI 故障的思考｜如何让 Kubernetes 更稳定？",
            "image": "https://static001.infoq.cn/resource/image/a3/64/a39399444061bca921c1b22fca42e264.jpg",
            "description": "<div><p>作者 | 阿里云容器服务高级技术专家 张维 (贤维)、阿里云容器服务技术专家 刘佳旭 (佳旭)</p><p></p><h2>从 OpenAI 集群故障谈起</h2><p></p><p>2024 年 12 月 11 日，OpenAI 出现了全球范围内的服务不可用故障，影响了 ChatGPT，API，Sora 等服务，故障持续时间超过四个小时，产生了严重影响。根据 OpenAI 在事后发布的故障报告 [1] ，此次故障的直接原因是升级监控组件导致 Kubernetes 集群控制面过载，然后因为数据面 CoreDNS 对控制面有强依赖导致影响应用服务，进一步放大了故障影响。（请参考详细故障报告 [2]）</p><p></p><p><img>https://static001.geekbang.org/wechat/images/89/89396cbd868b2658c52d690c74c0fd37.png<img></p><p></p><p>近年来，无论是在国内还是国际，我们都见证了许多自建 Kubernetes 集群遭遇故障的案例，这些故障为从事容器基础设施管理的人员提供了宝贵的学习机会。</p><p></p><p>从 Kubernetes 集群管理的视角，此次故障中值得被关注的技术问题有：</p><p></p><ul><li><p>OpenAI 自建 K8s 集群的单集群规模很大。</p></li></ul><ul><li><p>一个部署在节点侧的 Telemetry services 更新后，对 K8s 集群 API 控制面压力过大，导致控制面崩溃。</p></li></ul><ul><li><p>其 CoreDNS 对 K8s 集群控制面有强依赖，导致数据面服务受损。</p></li></ul><ul><li><p>K8s 集群控制面过载后无法操作集群，进而导致故障时间拉长。</p></li></ul><p></p><p>OpenAI 针对此次故障提出了几点后续要做的预防工作，在此我们就这些点进一步展开分析，并分享 ACK 在这些方面的稳定性实践经验。</p><p></p><h2>Kubernetes 稳定性基础实践 ：OpenAI 故障复盘</h2><p></p><h2>1. 健壮的分阶段部署 - Robust phased rollouts</h2><p></p><p>历史数据表明，故障大部分是由于变更导致的。灰度变更是基础设施稳定性要求的重要原则，通过强化灰度发布与所有基础设施变更的监控，控制故障爆炸半径并能及早发现。生产环境的变更发布需要严格遵守流程规范，尽量做到可灰度、可观测、可回滚。这里既包括多集群之间的分阶段部署（所有变更应先应用于测试和预发环境，留有一部分时间进行观察），同时也要求单集群内的变更也要尽量遵守可灰度原则。</p><p></p><p>灰度的过程主要遵循爆炸影响从轻到重，规模从小到大的步骤。灰度除了阶段控制外，尤为需要注意的是与可观测系统的联动。</p><p></p><p>社区 DaemonSet 的版本变更不支持灰度和分阶段部署，因此在进行变更时需格外谨慎，并持续监测对集群控制面和数据面的影响。为了解决社区 DaemonSet 缺乏灰度能力的问题，阿里云推出的 OpenKruise Advanced DaemonSet [3] 实现了可控的灰度发布功能，并提供多种滚动更新策略，以提升 DaemonSet 变更的稳定性。</p><p></p><h2>2. 紧急访问 Kubernetes 控制面 - Emergency Kubernetes control plane access</h2><p></p><p>控制面过载时，需要确保控制面的可操作性，进行限流等干预手段。</p><p></p><p>这里一方面是需要有针对 Kubernetes 控制面过载场景的更好应急限流手段，ACK 内部支持根据 ua/user/verb 等 client 多维度进行限流设置，从而通过更有效的限流快速控制过载情况。</p><p></p><p>另一方面是整体架构的设计，不同于自建 Kubernetes 集群通常多个 Master 节点独立部署架构（Apiserver 和 etcd 通也通常被部署在一个节点上），ACK 托管控制面采用容器化管理方式，在控制面的动态弹性伸缩和应急场景的可操作性可维护性，都带来更好的控制面管理优势。可参考下面介绍的“<strong>控制面架构形态选择</strong>”。</p><p></p><h2>3. 解耦数据面和控制面 - Decouple the Kubernetes data plane and control plane</h2><p></p><p>由于应用层依赖 Kubernetes DNS 进行服务发现，导致了与 Kubernetes 控制面存在耦合。在 Kubernetes 集群中数据面和控制面的解耦是一个重要的设计原则。在 ACK 环境中我们进行架构改造，使得 Kubernetes 数据面与控制面解耦。</p><p></p><p>除了 CoreDNS 依赖问题，其他 Kubernetes 应用也可能对控制面有强依赖。在此我们建议集群管理者应常态化对集群做故障演练，停止 Kubernetes 控制面以观察数据面业务服务是否有影响，包括服务间的 DNS 查询、Service 网络通信、服务响应等。（另外，ACK 提供 Managed CoreDNS 托管组件，降低用户对 CoreDNS 组件的稳定性管理负担）</p><p></p><p>ACK 不仅确保系统组件不对控制面有强依赖，同时对云服务整体架构上也是同样的原则。对 ACK 云服务自身的管控面和元集群控制面，我们也进行了常态化的演练，确保极端情况下这两者的异常不影响用户的托管 K8s 控制面。</p><p></p><h2>4. 故障注入测试 - Fault injection testing</h2><p></p><p>OpenAI 提到的故障注入测试与 ACK 持续开展的“<strong>故障演练</strong>”相对应。我们要强调，常态性的故障演练对维护基础设施的稳定性至关重要。ACK 通过在控制面和数据面持续开展故障演练，确保各层面的稳定性不降低，并不断增强快速恢复和应急处理的能力。故障演练体系的建设与实施并非一蹴而就，而是需要与日常研发和运维工作相结合，常态化实施，以实现对系统熵增的有效控制。</p><p></p><p><img>https://static001.geekbang.org/wechat/images/81/81c61647b7262804c7bcb088e363569c.png<img></p><p></p><p>针对 Kubernetes 体系产品，可以把可能的依赖故障拆解成以下几种类型。针对可能出现的依赖故障，逐步进行场景的设计，计划演练、常态演练甚至混沌演练，并且还需注意故障演练过程本身的风险控制如隔离准备、流量分离等。以下是一些分析入手的场景思路。</p><p></p><ul><li><p>控制面核心组件如 Apiserver 等负载所在基础设施的底层计算、存储、网络不可用。</p></li></ul><ul><li><p>主要网络存储组件 controller、webhook 等负载所在基础设施的底层计算、存储、网络不可用。</p></li></ul><ul><li><p>用户集群节点层面 Daemonset 或 kubelet 出现 Crash 或 OOM 等；</p></li></ul><ul><li><p>用户应用负载所在基础设施的底层计算、存储、网络不可用等；</p></li></ul><ul><li><p>控制面不可用的情况下，数据面受损程度（应当不受损）；</p></li></ul><ul><li><p>各组件对外资源依赖服务不可用、或返回异常；</p></li></ul><ul><li><p>集群控制资源规模、请求到达上限，或在上限的情况下进行集群整体批量运维动作（节点批量增减、负载迁移、组件批量升级重启等）；</p></li></ul><p></p><h2>5. 更快的恢复 - Faster recovery</h2><p></p><p>故障影响的衡量程度包括故障影响面和故障时长，通过更有效的应急可以快速控制故障时长，减少故障最终影响程度。这里可以通过常态化、场景化的故障演练持续提升快速恢复能力。</p><p></p><p>除了上述提到的稳定性实践外，我们注意到 OpenAI 采用了自建的大规模 Kubernetes 集群。借此机会，我们希望进一步探讨其他需要关注的 Kubernetes 稳定性问题和实践。同时，我们也将分享阿里云 ACK 在这一领域的思考与经验，以及自建 Kubernetes 集群与云服务提供商托管 Kubernetes 集群在长期管理上的一些差异。阿里云 ACK 已承载了数万个托管 Kubernetes 集群的控制面和数据面的全生命周期管理，积累了丰富的 Kubernetes 稳定性技术优化和实践经验，其中包含多个大规模集群的案例。</p><p></p><h2>Kubernetes 的其他稳定性挑战和实践</h2><p></p><h2>1. Kubernetes 技术复杂度高</h2><p></p><p><img>https://static001.geekbang.org/wechat/images/20/2052f1665fbae339181296189a71483c.png<img></p><p></p><p>K8s 控制面被视为 K8s 集群管理的核心“大脑”，其中包含了关键组件如 kv 数据存储 etcd、apiserver、scheduler 和 kube-controller-manager 等。尽管 Kubernetes 社区持续努力简化 K8s 的部署与管理，使得从零构建测试或学习环境变得相对容易，但长期维护和管理生产级 K8s 集群依然是一项极具挑战的工作。K8s 技术的复杂性较高，涵盖了多个领域，包括有状态组件 etcd 的 Raft 共识算法、Apiserver 的高可用性和缓存机制、证书管理等。当控制面出现异常时，往往需要具备专业知识和丰富经验的 K8s 技术专家，以便迅速定位和修复问题。</p><p></p><p><img>https://static001.geekbang.org/wechat/images/41/41050be4be0d22b28723e8907e1737c0.png<img></p><p></p><p>从另一个角度看，对于大部分集群管理者而言，是没有必要去了解 K8s 控制面和数据面技术细节的，而是更应该关注业务应用的部署和管理。</p><p></p><h2>2. 控制面架构形态选择</h2><p></p><p>传统自建 K8s 集群中控制面是采用多 Master 节点部署架构，这种架构让控制面的管理维护变得复杂和繁琐，让控制面高可用、弹性伸缩、故障应急的应对都更加复杂。我们看到这种形态下很多 K8s 集群 Master 节点自从创建之后就没有进行实质性的管理，进而导致在突发异常等情况下故障恢复和影响变得无法控制。</p><p></p><p>ACK 托管版集群 K8s 控制面是采用容器化的部署和管理方式，容器化云原生架构管理是 Cloud Scale 的稳定性架构基础，相比自建 K8s 集群 Master 多节点方式，以容器部署的托管控制面天然具有高度可扩展、可伸缩性、单副本容灾迁移、可运维操作性、灵活性、自动化运维等优势。</p><p></p><ul><li><p>面向韧性设计（Resistancy）</p></li></ul><ul><li><p>面向弹性设计（Elasticity）</p></li></ul><ul><li><p>面向自动化设计（Automation）</p></li></ul><p></p><p>ACK Apiserver/etcd 等托管组件都作为 Pod 运行在 K8s“元集群”中的数据面节点池中，通过技术手段保障不同租户的控制面得以单元化完全隔离。</p><p></p><p><img>https://static001.geekbang.org/wechat/images/6f/6f3c0930f499c2d9daf613afa181d9ce.png<img></p><p></p><h2>3. 高可用性挑战</h2><p></p><p>高可用设计是分布式系统的难题，K8s 控制面的高可用需要关注：</p><p></p><ul><li><p>etcd 的高可用（奇数节点，避免脑裂等）；</p></li></ul><ul><li><p>无状态组件（Apiserver/kube-scheduler 等）的高可用；</p></li></ul><ul><li><p>底层依赖资源和服务的高可用（ECS/SLB/OSS/ACR 等）</p></li></ul><p></p><p>对于大部分集群管理者而言，保障控制面在各种异常场景下的高可用是一件非常有挑战的工作。</p><p></p><p>自建 K8s 集群控制面往往以多 Master 节点方式部署，在可用区异常时的自动或者手动容灾迁移都是相对复杂的。而 ACK 托管控制面是基于容器化架构部署，在可用区异常时容灾迁移过程是更加简单且高效的，其可直接基于 K8s 原生 Pod 高可用语义设置。（ACK 托管集群控制面的 SLA 是 99.95%，参考 ACK 集群高可用架构推荐配置 [4]）</p><p></p><ul><li><p>多可用区的地域：所有托管组件均严格采用多副本、多可用区均衡打散部署策略，确保在单个可用区或节点发生故障时，集群仍然能够正常提供服务。</p></li></ul><ul><li><p>单可用区地域：所有托管组件均严格采用多副本、多节点打散部署策略，确保在单个节点发生故障时，集群仍然能够正常提供服务。</p></li></ul><p></p><p>控制面高可用保障是体系化的系统能力建设，不仅需要有良好的架构设计、完善的自动化运维体系应对容灾迁移场景，也包括在很多异常场景下的深度技术优化。</p><p></p><p>举例，当底层网络出现故障时，比如某些可用区之间可能相互连通，而其他可用区却无法连通，这容易导致 etcd 出现网络分区问题（Network Partition），从而引发 etcd “no leader”的情况。结合 Apiserver 的 HTTP/2 长连接机制，这种网络分区问题进而会导致节点变为 NotReady 状态。目前社区对 etcd 的网络分区问题尚未提供完善的解决方案，而 ACK Apiserver 针对此场景中进行了深度优化，通过更可靠的检查机制自动排除“no leader”的 etcd 后端。参考 etcd 设计网络分区问题 [5]。</p><p></p><h2>4. 大规模集群的可扩展性挑战</h2><p></p><p>K8s 已成为 AI 应用的重要基础设施，OpenAI 和 Google Cloud GKE 的单集群规模也在不断扩大。（参考 OpenAI 7500 nodes[6]，GKE 65K nodes [7]）</p><p></p><p><img>https://static001.geekbang.org/wechat/images/cc/cce39d8cd1179e01c70b0871125776bb.png<img></p><p></p><p>K8S 官方建议的最大集群规模是 5000 节点。然而，我们并不推荐自建 K8s 环境的用户将单集群扩展到如此大规模。但对于大多数集群管理者来说，确保大规模 K8s 集群控制面的稳定运行面临诸多挑战。例如，集群中大量节点的 kubelet、kube-proxy 和 DaemonSet 发起的 list-watch 请求可能会对控制面造成巨大压力，甚至引发服务风暴。而在 OpenAI 近期的故障中，正是由于 DaemonSet pod 产生的过载请求成为触发点。此外，K8s 在容器生态中作为分布式系统的 Universal Control Plane，尤其是在频繁使用自定义资源（CR）时，更容易导致控制面的崩溃。一旦控制面过载，客户端的重新连接请求会进一步加剧雪崩效应，OpenAI 的故障再次凸显了这一问题的严重性。</p><p></p><p>针对大规模场景，ACK 对 K8s 控制面进行了多方面的技术优化，以提升控制面的可扩展性。</p><p></p><ul><li><p><strong>控制面动态伸缩</strong>：Apiserver/etcd 通过弹性伸缩自适应集群规模，根据组件资源水位进行动态 HPA/VPA。</p></li></ul><ul><li><p><strong>Apiserver 限流增强：</strong></p></li></ul><ul><li><p>基于 QPS 的 ua limiter 限流：这种限流策略生效不依赖访问业务集群 Apiserver，过载场景下可根据配置策略快速阻止控制面流量。在 OpenAI 案例中这种限流能力对于故障恢复就会显得非常重要。</p></li></ul><ul><li><p>动态 APF 限制：ACK 集群默认支持标准 APF 限流能力，但社区 APF 限制不支持动态感知能力，在很多场景不是最优的限流策略。ACK Apiserver 根据资源水位动态调整 APF 最大并发限制。</p></li></ul><ul><li><p><strong>Apiserver 关键优化：</strong></p></li></ul><ul><li><p>提供组件的最优参数配置，并针对不同规模的集群设定相应参数。</p></li></ul><ul><li><p>优化 protobuf 和 json List 开销，提升 List 查询性能。</p></li></ul><ul><li><p>减少 Apiserver 推送 watch 事件的延迟。</p></li></ul><ul><li><p>降低数据面组件的 watch 请求，例如关闭 kube-proxy 对 headless service 的 watch，以节省网络带宽。</p></li></ul><ul><li><p><strong>Etcd 关键优化：</strong></p></li></ul><ul><li><p>Data 和 Event etcd 分拆：Data 和 Event 存放到不同的 etcd 集群，数据和事件流量分离，消除事件流量对数据流量的影响。</p></li></ul><ul><li><p>自动碎片整理（AutoDefrag）：etcd operator 监控 etcd db 使用情况，自动触发碎片整理，降低 db 大小，提高查询速度。</p></li></ul><ul><li><p>etcd 内核增强：对热点 key 和大象 key 进行跟踪监控。</p></li></ul><ul><li><p>Raft 积压问题优化：优化大规模集群中常常出现的 raft 积压问题。</p></li></ul><p></p><p>通过这些技术优化，ACK 能够平稳支持线上大规模集群的稳定运行，在某些场景下单个集群可承载 15K 节点和 5 万 ECI Pod。如需了解更多规模化集群的实践，请参考 ACK 大规模集群使用建议 [8]。</p><p></p><h2>5. 持续运维的挑战</h2><p></p><p>K8s 集群的管理是一个长期且持续的复杂任务，贯穿集群的整个生命周期，生命周期内需要持续的监控、更新和升级。</p><p></p><p><strong>a. Kubernetes 版本升级</strong></p><p></p><p>K8s 的设计决定了在升级时默认不支持回滚到低版本，如何在确保集群稳定运行的前提下，平滑的升级自建 K8s 控制面，是集群管理者面临的一项重大挑战。</p><p></p><p>这几年我们常常看见业界自建 K8s 集群出现多起升级故障，而云上 K8s 托管集群基本没有见到类似的故障报告，其关键原因可以总结为两点：</p><p></p><ul><li><p>自建 K8s 集群 Master 节点的升级复杂度是远远高于容器化管理的托管 K8s 控制面升级。自建集群以多个 Master 节点依次升级，每个 Master 节点内部需要做的升级操作复杂度较高，不仅要考虑各种 Manifest 文件的配置覆盖，也需要考虑 etcd 版本升级，还需要考虑 Master 节点 kubelet 的升级。而因为托管 K8s 控制面简洁的容器化管理架构，其升级过程是极大简化的，这种架构层面的简化会大大提升 K8s 控制面升级的稳定性。</p></li></ul><ul><li><p>云厂商提供更完善的产品化机制保障升级可靠性。云厂商对于 K8s 升级的可靠性极度重视，相比自建 K8s 集群，会提供更严谨更完善的升级前置检查体系，包括组件兼容性检查、废弃 API 检查等，提供让用户无感的自动 etcd 数据备份机制。另外 ACK 提供自动升级能力，进一步降低 K8s 升级的负担。参考 ACK 版本机制 [9] 和 ACK 自动升级 [10]。</p></li></ul><p></p><p><strong>b. Kubernetes 可观测性系统</strong></p><p></p><p>Kubernetes 可观测性（Observability）是集群管理的重要基础能力，它使集群管理者能够检查服务的可用性和可靠性、进行故障诊断与排除、优化性能以及进行容量规划等。由于 K8s 环境的动态性和复杂性，构建一个独立的可观测性系统并进行持续监测的难度较大。ACK 在如何构建一个稳定的 K8s 可观测性系统方面有几点长期实践经验。</p><p></p><p></p><p><strong>c. 证书过期和轮转</strong></p><p></p><p>证书过期是 K8s 环境常见的故障原因，K8s 证书体系本身较为复杂，有 Apiserver 证书、etcd 证书、kubelet 证书、集群 CA 证书、kubeconfig 凭据等，这里一方面要求集群管理者需要无遗漏的进行全集群证书可用性监控，另一方面要求能够平滑稳定的进行证书轮转操作。云厂商托管 K8s 集群会全面负责这些基础证书管理，而不用用户关心。而在自建 K8s 集群中需要用户重视证书的管理问题。</p><p></p><h2>6. 安全性的挑战</h2><p></p><p>在云计算行业，安全作为一个横向技术领域，需要企业投入较大的成本构建和培养同时具备云原生 K8s 和云安全等多个专业领域知识的安全运维团队。在 K8s 控制面安全方面，企业安全运维需要遵循最佳安全实践配置集群控制面参数，同时持续监控和应对 CVE 漏洞、并处理 kubeconfig 凭据和密钥的生命周期管理、以及是否最小化授权等安全问题。此外企业还需要基于 DevSecOps 原则构建内部供应链安全，保证企业应用从开发构建到部署运行的生命周期中，每一步的关键流程都是在安全规范的指导下进行，并满足行业合规要求。</p><p></p><h2>7. 基础设施稳定性和安全性的责任共担</h2><p></p><p>在云上托管 Kubernetes 集群的基础设施稳定性和安全性中，云厂商与用户的责任是相辅相成、不可分割的。云厂商负责提供可靠的基础设施、全面的安全保障以及高可用性的服务架构，以确保集群的运行环境稳定、弹性和安全。用户则需合理正确的对集群进行配置、监控和管理，实施最佳实践，及时更新维护集群和应用，应对潜在的安全威胁和性能瓶颈。在这种共担责任模型中，才可实现高效稳定的 Kubernetes 集群环境，最大程度地提升业务的连续性与可靠性。</p><p></p><p><img>https://static001.geekbang.org/wechat/images/58/58f71871c051ac8dcaba18bfc5e5ff39.png<img></p><p></p><h2>写在最后</h2><p></p><p>Kubernetes 已成为云原生时代的重要基础设施，管理庞大复杂的基础设施从来都不是一件容易的事情。很多领先的技术型公司，也曾遭遇自建 Kubernetes 故障。在超大规模 Kubernetes 集群运维管理工作进入深水区的过程中，所有企业都面临着相同的故障风险和技术难题。</p><p></p><p>“Complexity has to live somewhere.” （复杂度是不灭的，只会转移）</p><p></p><p>云计算提供商在服务千行百业的客户过程中，一直在努力实现技术突破和积累工程化经验，并将这些突破和经验转化为标准化的产品服务。通过降低企业客户的运维复杂度，减少各类故障并帮助客户缩短业务恢复时间和降低影响程度，让企业能够更加关注业务创新。对于大部分企业，云服务或许是一个更优选择。</p><p></p><p>同时，经验的积累没有压缩算法。云计算也在经验中成长，努力为客户提供更稳定的基础设施服务，谨以此文与大家共勉。</p><p></p><p><strong>相关链接：</strong></p><p>[1] 故障报告https://status.openai.com/incidents/ctrsv3lwd797</p><p>[2] 详细故障报告https://status.openai.com/incidents/ctrsv3lwd797</p><p>[3] OpenKruise Advanced DaemonSethttps://openkruise.io/docs/next/user-manuals/advanceddaemonset/</p><p>[4] ACK 集群高可用架构推荐配置https://help.aliyun.com/zh/ack/ack-managed-and-ack-dedicated/user-guide/cluster-high-availability-architecture-recommend-configuration</p><p>[5] etcd 设计网络分区问题https://etcd.io/docs/v3.5/learning/design-client/</p><p>[6] OpenAI 7500 nodeshttps://openai.com/index/scaling-kubernetes-to-7500-nodes/</p><p>[7] GKE 65K nodeshttps://cloud.google.com/blog/products/containers-kubernetes/gke-65k-nodes-and-counting</p><p>[8] ACK 大规模集群使用建议https://help.aliyun.com/zh/ack/ack-managed-and-ack-dedicated/user-guide/suggestions-on-how-to-work-with-large-ack-pro-clusters</p><p>[9] ACK 版本机制https://help.aliyun.com/zh/ack/ack-managed-and-ack-dedicated/user-guide/support-for-kubernetes-versions</p><p>[10] ACK 自动升级https://help.aliyun.com/zh/ack/ack-managed-and-ack-dedicated/user-guide/automatically-upgrade-an-ack-cluster</p></div>",
            "link": "https://www.infoq.cn/article/SiC0YrEuk0pbCwPavWb2",
            "pub_date": "2025-01-16 10:50:36",
            "source": "infoq",
            "kind": 1,
            "language": "zh-CN"
        },
        {
            "id": "uFXJ59JlLH3YDldi9NWl",
            "title": "音视频场景破局，视频云技术开启新浪潮",
            "image": "https://static001.infoq.cn/resource/image/08/5b/088a7f5b2b77f6e4bf4bb848b4fd515b.jpg",
            "description": "<div><p><strong>AI改变的不仅是音视频的创作方式，而在重新定义人们的交互与消费模式。</strong></p><p></p><p>近年来，随着大模型的引入，音视频消费场景正迎来新的机遇。与过去相比，智能语音助手更加智能化，且能提供更为即时和个性化的响应；视频平台也开始依据用户的历史偏好，自动生成符合个人兴趣的视频片段；在赛事直播方面，观众可以通过VR设备获得全新的视角，并借助AI技术分析球员的表现。</p><p></p><p>这些新兴场景不仅对创新能力提出了更高的要求，同时也考验着视频云技术的底层支持能力。作为基础设施，视频云不能只是支撑内容的生成、处理、传输及消费，还需进一步提升以满足更加复杂的需求。</p><p></p><p>除了娱乐行业，教育、医疗和金融等专业领域也可能涌现出更为复杂的应用场景，这对视频云技术的发展方向提出了新的指引，预示着更具挑战性的未来机遇。</p><p></p><p>其中火山引擎在视频云与AI融合的实践中处于行业前沿。在中，深入探讨了AI时代下视频云技术的革新与应用，展现了火山引擎视频云在音视频全链路中的创新实践，以及AIGC技术如何赋能音视频领域的新发展。</p><p></p><p>本文将继续跟随的脚步，深入探讨视频云技术创新所带来的无限可能性。在面对AI Bot的语音交互障碍，如何提供更加自然流畅的人机对话体验？面对日益增长的多媒体处理需求，火山引擎如何不断优化自身的架构设计和技术能力？在视频直播领域，全新技术的引入是否带来新的可能性？</p><p></p><h2>让AI Bot从“能说话”到“会说话”</h2><p></p><p>近年来，AI Bot作为一项前沿技术，正迅速成为人工智能落地的热门途径。从智能客服、办公助手，到情感陪伴机器人，其应用范围正在持续拓展。然而，在语音交互场景中，当前的AI Bot面临语音识别不准确、语义理解浅显及反馈机械呆板等问题，“已读乱回”现象正在严重影响用户体验。</p><p></p><p><strong>在此背景下，扣子平台推出了全新的智能语音OpenAPI，并接入了</strong><strong>（Real-Time Communication）能力，旨在打造一个更加贴近人类真实交流体验的AI生态。</strong></p><p></p><p>RTC是一种支持实时语音、视频和数据传输的技术框架，它使用高效的RTP协议并结合多种算法来确保数据的快速、稳定传递。相较于传统的WebSocket技术，RTC在抗弱网能力、全双工通信等方面具有显著优势。例如，在网络条件不佳的情况下，RTC仍能保持高质量的通话质量；同时，它允许说话者和听者在同一时间发送和接收信息，使得AI Bot可以实现实时打断功能，极大地提升了用户体验的真实性和互动性。</p><p>扣子作为热门的AI Bot开发平台，以其易用性和灵活性饱受开发者好评。用户无需编写代码即可创建个性化的聊天机器人，并且可以轻松部署到不同的平台或应用程序中。此次引入RTC技术，扣子不仅增强了其语音识别和合成的能力，还实现了毫秒级响应时间和流式输入特性，让AI Bot从“能说话”真正升级到“会说话”。此外，RTC技术的加入也为扣子带来了更强的网络适应性和稳定性，即使在网络环境不稳定时也能保证良好的通话效果。</p><p></p><p>火山引擎视频云RTC技术已经在抖音、飞书等多个平台上得到了广泛应用，证明了其可靠性和高效性。特别是在处理高并发请求方面，火山引擎视频云RTC技术表现优异，这得益于自研算法、精细化设备适配以及强大的云端算力支持。</p><p></p><p>RTC技术的应用将使AI Bot在未来的企业客户服务和个人消费市场中发挥更重要的作用。企业可以通过定制化设置来构建高度拟人化的客服机器人，提高工作效率和服务质量；而在C端市场，如游戏、教育等领域，实时语音交互将带来更加沉浸式的用户体验。此外，结合智能硬件的发展，AI Bot还可以应用于智能家居等场景，进一步拓展其功能性和应用场景。未来，随着多模态能力的提升，AI Bot将在视觉、听觉等多种感官上实现有机结合，为用户提供更多元化的服务选择。</p><p></p><p>扣子平台与RTC技术的结合不仅是技术上的突破，更是推动AI应用实践落地的重要一步。它不仅满足了当前用户对于高质量语音交互的需求，更为未来的创新和发展奠定了坚实的基础。</p><p></p><h2>端侧处理兴起，轻量级框架正当时</h2><p></p><p>随着AI Bot等AI应用技术的不断创新，用户体验和服务模式日益丰富和复杂化，这不仅推动了应用层的革新，也对支撑其运行的基础设施也提出了更高的要求。</p><p></p><p>目前，视频生成大模型的训练规模与预处理计算需求呈指数级增长，市场不仅需要确保这些大模型能够在云端高效运行，还要求端侧也需具备一定的音视频处理能力，以便与云端共同应对复杂的计算挑战。</p><p></p><p><strong>面对以上需求，火山引擎推出全新端侧媒体处理框架——</strong><strong>。BMF Lite是火山引擎基于BMF自研端侧的通用的多媒体框架的的轻量化版本，历经三年打磨，目前已应用于抖音、西瓜视频等应用的主要业务场景中，涵盖播放、推流、图片处理和云游戏等领域。该框架横跨Android、iOS、鸿蒙、PC和Web等多个平台，服务于超过十亿用户，每日处理数万亿次的视频和图片请求。</strong></p><p></p><p>在框架层，BMF Lite强调跨平台兼容性和资源的有效复用。它采用了统一的数据结构设计，确保了Android、iOS、Web以及PC等多平台的支持。为了应对资源受限的问题，BMF Lite引入了算法控制器来管理算法实例的生命周期，并通过资源池机制实现了不同算法间算子和数据资源的共享。这一设计减少了频繁创建和销毁资源所带来的开销，特别适用于点播和直播后处理场景，在抖音播放中显著提升了资源利用率。</p><p></p><p>BMF Lite还扩充了客户端的一些异构能力，涵盖了DSP、NPU以及端侧GPU等多种计算单元。这不仅提高了计算效率，也为开发者提供了更多的选择，可以根据具体应用场景灵活调配计算资源。</p><p></p><p>随着AI视频时代的到来，强大的视频生成大模型成为必要，但其训练面临成本、质量、协同和性能等多方面挑战。BMF通过与字节大模型团队合作，针对海量视频数据进行高效预处理，短时间内生成了大量高质量素材，支撑视频生成模型的训练、上线及调优。为应对成本挑战，我们采用潮汐资源和精细化混部调度；质量上，通过30多种算子对视频进行多维度分析筛选；协同方面，BMF动态模块特性加速了算子集成与链路开发，效率数倍于传统框架；性能优化中，BMF灵活调度CPU、GPU、ARM等资源，实现了快速性能调优，显著提升了任务吞吐量并缓解了资源瓶颈。</p><p></p><p>未来，火山引擎计划推出基于BMF的大模型视频预处理方案，该方案将为大模型企业提供一种灵活且低成本的视频预处理服务。企业能够以更经济的方式获取高质量的视频数据支持，加速其模型训练过程并提升最终模型的表现。</p><p></p><h2>“全景式”的直播时代来临</h2><p></p><p>随着底层技术的进步和AI开发的日益丰富，越来越多视频形式在应用场景中涌现，其中“全景式”的直播体验正逐渐吸引人们的广泛关注，视频行业正在向更加沉浸式和个性化的方向迈进。</p><p></p><p>在视频通讯方面，Google的Starline项目提供了一种如同面对面交流般自然且逼真的视频通话体验，用户甚至能通过身体移动或眼神接触增强沟通效果，这项技术为传统的2D视频会议带来了全新的挑战；而在VR领域，像Pico这样的头显设备则为用户打造了一个完全沉浸式的虚拟环境，允许他们在目标场景中自由探索和漫游。这些科幻电影般的场景，其背后的核心力量便是六自由度视频技术，从二维到三维视觉体验的重大飞跃，为各行各业开辟了创新的机会和发展空间。</p><p></p><p>与传统的360度全景视频不同，六自由度视频允许用户在三维空间内进行全方位的移动和旋转，包括前后、左右、上下三个维度的平移以及偏航、俯仰、侧倾三种形式的转动。观众能够主动根据个人喜好选择最佳观赏位置，获得身临其境般的沉浸式体验。</p><p></p><p>近年来，六自由度视频技术的应用范围也在不断扩大。其中，<strong>火山引擎视频云所推出的</strong><strong>便是对六自由度视频的新升级，有望成为未来数字娱乐与信息传播的重要组成部分。</strong></p><p></p><p>六自由度（6DoF）直播技术通过从数据采集到云端重建、编解码再到端侧渲染，构建了一条完整的处理链路，以此提供沉浸式的多视角观看体验。</p><p></p><p>在数据采集阶段，多相机系统获取的视频流经过时间同步与聚合编码，形成一路推流至CDN，并转发给云端服务器。</p><p></p><p>云端处理分为两个主要步骤：第一步是通过训练的编码器提取人体特征，利用correlation volume进行多视角图像匹配，恢复深度图，并借助多头注意力机制在稀疏视角下重建初步的人体点云模型。第二步则生成精细的3DGS模型，预测包括透明度、尺度和旋转在内的高斯属性，学习不同姿态下的参数分布，自适应调整高斯体分布以减少点数，从而渲染出逼真图像。</p><p></p><p>为了适配现有视频传输链路，火山引擎开发了一套轻量高效的3DGS模型压缩编码算法，将模型流转化为视频流推送至CDN。为保证解码后模型的渲染效果，依据不同高斯属性分配差异化码率，优化传输与渲染质量。播放端从CDN拉取并实时解码视频数据，还原三维模型进行渲染，支持手机、PC和VR头显等多平台实时交互观看。在云端，使用Nvidia Turing及以上显卡（如3090）可实现实时重建；播放端如iPhone 15能实现30FPS以上、个人电脑60FPS以上的实时解码与渲染帧率，图像PSNR达30dB以上，整体延迟能控制在200ms以内，确保几乎无延迟的直播体验。</p><p></p><p>目前，六自由度直播技术已经成功落地火山引擎视频云的直播服务中。未来，这项技术将会推动更多元化的直播内容出现。</p><p></p><p>在，我们不仅见证了AI Bot技术从基础语音交互向高度拟人化交流的重大跨越，也揭示了多媒体处理框架和六自由度直播技术在提升用户体验方面的重要作用。随着这些前沿科技的深入应用，可以预见未来视频云技术的进步将会不断创造全新的可能性，重新定义我们与世界互动的方式。</p></div>",
            "link": "https://www.infoq.cn/article/uFXJ59JlLH3YDldi9NWl",
            "pub_date": "2025-01-15 06:54:56",
            "source": "infoq",
            "kind": 1,
            "language": "zh-CN"
        }
    ]
}